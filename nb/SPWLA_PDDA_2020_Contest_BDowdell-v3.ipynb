{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"top\"></a>SWPLA PDDA 2020 Synthetic Sonic Log Generation Contest\n",
    "\n",
    "**Author:** Ben Dowdell\n",
    "\n",
    "**Date:** March XX, 2020\n",
    "\n",
    "**Purpose:** To minimize RMSE in predicting Sonic logs (compressional & shear) from a suite of standard well logs\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [1. Initial Set-Up](#initial-setup)\n",
    "** [1a. Standard Imports](#standard-imports)\n",
    "** [1b. Sklearn Imports](#sklearn-imports)\n",
    "** [1c. Helper Function Definitions](#helper-funcs)\n",
    "* [2. Read Data](#read-data)\n",
    "* [3. Inspect Data](#inspect-data)\n",
    "* [4. EDA](#eda)\n",
    "* [5. Imputing Missing Values](#imputing)\n",
    "** [5a. PE estimation](#fe-pe)\n",
    "** [5b. DTC approximation](#fe-dtc)\n",
    "** [5c. DTS approximation](#fe-dts)\n",
    "\n",
    "### Data Decription\n",
    "#### Files\n",
    "#### train.csv\n",
    "All the values equals to -999 are marked as missing values.\n",
    "- CAL - Caliper, unit in Inch,  \n",
    "- CNC - Neutron, unit in dec \n",
    "- GR - Gamma Ray, unit in API\n",
    "- HRD - Deep Resisitivity, unit in Ohm per meter,\n",
    "- HRM - Medium Resistivity, unit in Ohm per meter,\n",
    "- PE - Photo-electric Factor, unit in Barn,\n",
    "- ZDEN - Density, unit in Gram per cubit meter, \n",
    "- DTC - Compressional Travel-time, unit in nanosecond per foot,\n",
    "- DTS - Shear Travel-time, unit in nanosecond per foot,\n",
    "\n",
    "\n",
    "#### test.csv\n",
    "The test data has all features that you used in the train dataset, except the two sonic curves DTC and DTS.\n",
    "\n",
    "####  sample_submission.csv\n",
    "A valid sample submission.\n",
    "<p><font style=\"\">\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"initial-setup\"></a>1. Initial Set-Up\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"standard-imports\"></a> 1a. Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from pprint import pprint\n",
    "import pandas_profiling as pdp\n",
    "\n",
    "random_state = 42\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sklearn-imports\"></a>1b. Sklearn Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing utilities\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering Models\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Visualization\n",
    "from yellowbrick.model_selection import FeatureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"helper-funcs\"></a>1c. Helper Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot data histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distributions(df):\n",
    "    \"\"\"Plot histograms of each curve\n",
    "    \n",
    "    Paramters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,8))\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df[col])\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function plot data CDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_cdf(df):\n",
    "    \"\"\"Plot cumulative distribution function of each curve\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,8))\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df[col], density=True, cumulative=-1)\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot well curves in a normal log display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_well_curves(data):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : Input data frame containing the well log curves, one per column\n",
    "    \n",
    "    curve_names (list) : A list containing the column name of each well log curve in the input data frame\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # get the column names as a list\n",
    "    curve_names = data.columns.tolist()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data[curve], data.index, color='k')\n",
    "        else:\n",
    "            ax.plot(data[curve], data.index, color='k')\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS'] or 'DT' in curve:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Short Time Fourier Transform of the well curves (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stft_well_curves(data):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    Overlays the well log curve on top of a STFT representation of the signal\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : Input data frame containing the well log curves, one per column\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # get the column names as a list\n",
    "    curve_names = data.columns.tolist()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        f, t, Zxx = signal.stft(data[curve], fs=1)\n",
    "        ax.pcolormesh(f.T, t.T, np.abs(Zxx).T, cmap='viridis', vmin=0, vmax=0.5)\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax1 = ax.twiny()\n",
    "            ax1.semilogx(data[curve], data.index, color='k')\n",
    "        else:\n",
    "            ax1 = ax.twiny()\n",
    "            ax1.plot(data[curve], data.index, color='k')\n",
    "        if curve == 'CNC':\n",
    "            ax1.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.invert_xaxis()\n",
    "            ax1.set_title(curve, fontdict={'color':'r'})\n",
    "            ax1.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax1.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to remove outliers from log curves using standard deviation from median value in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_curves(data, window, n_std):\n",
    "    \"\"\"\n",
    "    Takes a data frame containing well log curves and filters outliers based on n-standard deviations\n",
    "    from a median filtered version of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : The input data frame containing data to be filtered.  Each column is a well log curve.\n",
    "    \n",
    "    window (int) : The size of the window to use in creating a median filtered curve (recommend 33)\n",
    "    \n",
    "    n_std (int) : The number of +/- standard deviations to use in considering outliers (recommend 2)\n",
    "    \n",
    "    Returns:\n",
    "    df_clean (pandas.DataFrame) : The filtered well curves\n",
    "    df_outliers (pandas.DataFrame) : The outlier data points removed by the filtering operation\n",
    "    \"\"\"\n",
    "    # create a copy of the original data\n",
    "    df_copy = data.copy()\n",
    "    \n",
    "    # create a data frame containing median-filtered version of the data\n",
    "    df_medfilter = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_medfilter[col] = df_copy[col].rolling(window, min_periods=1, center=True).median()\n",
    "        \n",
    "    # create a data frame containing standard deviation of the data\n",
    "    df_stddev = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_stddev[col] = df_medfilter[col].rolling(window, min_periods=1, center=True).std()\n",
    "        \n",
    "    # create a data frame containing the cleaned version of the data using standard deviation from the median filtered data\n",
    "    # and create a data frame containing the removed outliers\n",
    "    df_clean = pd.DataFrame()\n",
    "    df_outliers = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        upper = df_medfilter[col] + df_stddev[col]*n_std\n",
    "        lower = df_medfilter[col] - df_stddev[col]*n_std\n",
    "        df_clean[col] = df_copy[col].where((df_copy[col] <= upper) & (df_copy[col] >= lower))\n",
    "        #df_clean[col] = df_clean[col].interpolate(limit_area='inside')\n",
    "        df_outliers[col] = df_copy[col].where(df_clean[col] != df_copy[col])\n",
    "    \n",
    "    return df_clean, df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare raw versus filtered logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_curve_filt(data_raw, data_cleaned, *args):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data_raw (pandas.DataFrame) : Input data frame containing the well log curves, one log per column\n",
    "    \n",
    "    data_cleaned (pandas.DataFrame) : Input data frame containing filtered log curves, one log per column\n",
    "    \n",
    "    *args (pandas.DataFrame) : Optional data frame containing outliers\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the column names as a list, assumes col names are the same for each input data frame\n",
    "    curve_names = data_raw.columns.tolist()\n",
    "    \n",
    "    # build the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.semilogx(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.semilogx(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        else:\n",
    "            ax.plot(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.plot(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.plot(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare individual predictors to DTC & DTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_curves(predictor, dtc, dts):\n",
    "    \"\"\"\n",
    "    Takes a predictor curve and plots it against both response variables, DTC & DTS\n",
    "    \n",
    "    Parameters:\n",
    "    predictor (pandas.core.series.Series) : Independent Variable to compare, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dtc (pandas.core.series.Series) : DTC curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dts (pandas.core.series.Series) : DTS curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,10), linewidth=5, edgecolor='k')\n",
    "    name = predictor.name\n",
    "    fig.suptitle('{} curve comparison to DTC & DTS'.format(name), fontsize=20)\n",
    "    \n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    if name == 'HRD':\n",
    "        ax1.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax1.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax1c = ax1.twinx()\n",
    "    ax1c.plot(dtc.index, dtc.values, 'r', lw=2)\n",
    "    ax1c.invert_yaxis()\n",
    "    ax1.set_ylabel(name)\n",
    "    ax1c.set_ylabel('DTC', fontdict={'color':'r'})\n",
    "    ax1.grid(False)\n",
    "    ax1c.grid(False)\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2 = sns.scatterplot(x=predictor, y=dtc)\n",
    "    if name == 'HRD':\n",
    "        ax2.set_xscale('log')\n",
    "    ax2.set_xlabel(name)\n",
    "    ax2.set_ylabel('DTC')\n",
    "    ax2.grid(False)\n",
    "    \n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    if name == 'HRD':\n",
    "        ax3.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax3.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax3c = ax3.twinx()\n",
    "    ax3c.plot(dts.index, dts.values, 'r', lw=2)\n",
    "    ax3c.invert_yaxis()\n",
    "    ax3.set_ylabel(name)\n",
    "    ax3c.set_ylabel('DTS', fontdict={'color':'r'})\n",
    "    ax3.grid(False)\n",
    "    ax3c.grid(False)\n",
    "    \n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    ax4 = sns.scatterplot(x=predictor, y=dts)\n",
    "    if name == 'HRD':\n",
    "        ax4.set_xscale('log')\n",
    "    ax4.set_xlabel(name)\n",
    "    ax4.set_ylabel('DTS')\n",
    "    ax4.grid(False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate Root Mean Squared Error (RMSE), the primary evaluation metric for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(real, predicted):\n",
    "    \"\"\"Calculates the Root Mean Square Error\n",
    "     \n",
    "     Parameters:\n",
    "     real (ndarray) : actual values as a numpy array\n",
    "     predicted (ndarray) : predicted values as a numpy array\n",
    "     \n",
    "     Returns: RMSE Accuracy as a float\n",
    "     \n",
    "    \"\"\"\n",
    "    mse = np.square(np.subtract(real, predicted)).mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot logs in a typical petrophysical layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def petro_plot(gr, resd, neut, den, pe, dtc, dts):\n",
    "    \"\"\"\n",
    "    Produces a typical petrophysical plot\n",
    "    \n",
    "    Parameters:\n",
    "    gr (pandas.Series) : Gamma Ray curve as a pandas series\n",
    "    resd (pandas.Series) : Deep Resistivity curve as a pandas series\n",
    "    neut (pandas.Series) : Neutron Porosity curve as a pandas series\n",
    "    den (pandas.Series) : Density curve as a pandas series\n",
    "    pe (pandas.Series) : PE curve as a pandas series\n",
    "    dtc (pandas.Series) : Compressional sonic curve as a pandas series\n",
    "    dts (pandas.Series) : Shear sonic curve as a pandas series\n",
    "    \n",
    "    Returns : matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "    # GR\n",
    "    ax1 = fig.add_subplot(1,5,1)\n",
    "    ax1.plot(gr, gr.index, 'k', lw=1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('GR')\n",
    "    ax1.set_ylabel('Sample no.')\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.set_xlim([0, 300])\n",
    "    ax1.set_xticks([0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300])\n",
    "    ax1.xaxis.set_label_position('top')\n",
    "\n",
    "    # RD\n",
    "    ax2 = fig.add_subplot(1,5,2)\n",
    "    ax2.semilogx(resd, resd.index, 'b', lw=1)\n",
    "    ax2.set_xlabel('HRD')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.xaxis.tick_top()\n",
    "    ax2.xaxis.set_label_position('top')\n",
    "\n",
    "    # N-D\n",
    "    ax3 = fig.add_subplot(1,5,3)\n",
    "    ax3.plot(den, den.index, 'r', lw=1)\n",
    "    ax3.xaxis.set_label_position('top')\n",
    "    ax3.set_xlabel('ZDEN', labelpad=10, fontdict={'color':'r'})\n",
    "    ax3.set_xlim(1.65, 2.65)\n",
    "    ax3.invert_yaxis()\n",
    "    ax3b = ax3.twiny()\n",
    "    ax3b.plot(neut, neut.index, 'g', lw=1)\n",
    "    ax3b.invert_xaxis()\n",
    "    ax3b.set_xlabel('CNC', fontdict={'color':'g'})\n",
    "    ax3b.set_xlim(0.6, 0.0)\n",
    "    ax3b.xaxis.tick_top()\n",
    "    ax3.xaxis.tick_top()\n",
    "    ax3.set_xticks([1.65, 1.85, 2.05, 2.25, 2.45, 2.65])\n",
    "    ax3.tick_params(axis='x', pad=35, top=True)\n",
    "    ax3b.xaxis.set_label_position('top')\n",
    "    ax3b.set_xticks([0.6, 0.48, 0.36, 0.24, 0.12, 0.0])\n",
    "\n",
    "    # PE\n",
    "    ax4 = fig.add_subplot(1,5,4)\n",
    "    ax4.plot(pe, pe.index, 'k', lw=1)\n",
    "    ax4.set_xlabel('PE')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "\n",
    "    # Sonic\n",
    "    ax5 = fig.add_subplot(1,5,5)\n",
    "    ax5.plot(dtc, dtc.index, 'b', lw=1)\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    ax5.set_xlabel('DTC', labelpad=10, fontdict={'color':'b'})\n",
    "    ax5.set_xlim(50, 170)\n",
    "    ax5.invert_xaxis()\n",
    "    ax5.invert_yaxis()\n",
    "    ax5b = ax5.twiny()\n",
    "    ax5b.plot(dts, dts.index, 'r', lw=1)\n",
    "    ax5b.invert_xaxis()\n",
    "    ax5b.set_xlabel('DTS', fontdict={'color':'r'})\n",
    "    ax5b.set_xlim(340, 100)\n",
    "    ax5b.xaxis.tick_top()\n",
    "    ax5.xaxis.tick_top()\n",
    "    #ax5.set_xticks([170, 150, 130, 110, 90, 70, 50])\n",
    "    ax5.set_xticks([230, 200, 170, 140, 110, 80, 50])\n",
    "    ax5.tick_params(axis='x', pad=35, top=True)\n",
    "    ax5b.xaxis.set_label_position('top')\n",
    "    #ax5b.set_xticks([340, 300, 260, 220, 180, 140, 100])\n",
    "    ax5b.set_xticks([460, 400, 340, 280, 220, 160, 100])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot RHOMAA-UMAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rhomaa_umaa(pe, dens, phi, color_var=None):\n",
    "    \"\"\"Plot log data using RHOMAA-UMAA crossplot for mineralogy\n",
    "    \n",
    "    Parameters:\n",
    "    pe (ndarray-like) : PE curve\n",
    "    dens (ndarray-like) : Density curve\n",
    "    phi (ndarray-like) : Neutron porosity curve\n",
    "    color_var (optional) : Default 'None' will use index as color.  Option to pass in labels such as kmeans.labels_\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # First, create the endmember points as a dataframe\n",
    "    index = ['Quartz', 'Calcite', 'Dolomite', 'Anhydrite', 'K-Feldspar']\n",
    "    data = {\n",
    "        'rhoma' : [2.65, 2.71, 2.87, 2.95, 2.54],\n",
    "        'uma' : [4.82, 13.79, 8.98, 14.99, 7.29],\n",
    "    }\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    \n",
    "    # Second, calculate umaa & rhomaa for the input logs\n",
    "    u = pe * dens\n",
    "    phid = (2.65 - dens)/(2.65-1.)\n",
    "    phit = (phid + phi)/2\n",
    "    #umaa = u / (1 - phit)\n",
    "    #umaa = (u - phit*0.5)/(1-phit)/2\n",
    "    rhomaa = (dens - phit*1.)/(1-phit) # assume brackish pore fluid rhof = 1.1 g/cc\n",
    "    umaa = (pe * rhomaa) / 2\n",
    "    \n",
    "    # Finally, create and return figure\n",
    "    fig = plt.figure(figsize=(28,10))\n",
    "    \n",
    "    gs = fig.add_gridspec(1, 6)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0,:-3])\n",
    "    if type(color_var) == None:\n",
    "        im = ax1.scatter(umaa, rhomaa, s=10, c=umaa.index, marker='.', cmap='inferno', alpha=0.8)\n",
    "    else:\n",
    "        im = ax1.scatter(umaa, rhomaa, s=10, c=color_var, marker='.', cmap='inferno', alpha=0.8)\n",
    "    ax1.scatter(df['uma'], df['rhoma'], c='r', marker='D')\n",
    "    for i, txt in enumerate(df.index.tolist()):\n",
    "        ax1.annotate(txt, (df['uma'].iloc[i]+0.03, df['rhoma'].iloc[i]-0.02), weight='bold')\n",
    "    ax1.plot(df['uma'].loc[['Quartz','Calcite']], df['rhoma'].loc[['Quartz', 'Calcite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Quartz', 'Dolomite']], df['rhoma'].loc[['Quartz', 'Dolomite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Dolomite','Calcite']], df['rhoma'].loc[['Dolomite','Calcite']], 'k')\n",
    "    ax1.set_xlabel('UMAA (barns/cc)')\n",
    "    #ax1.set_xlim(2,16)\n",
    "    ax1.set_ylabel('RHOMAA (g/cc)')\n",
    "    #ax1.set_ylim(2.3, 3.1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title('RHOMAA-UMAA Crossplot', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "    cbar = fig.colorbar(im, ax=ax1)\n",
    "    cbar.ax.invert_yaxis()\n",
    "    cbar.set_label('sample no', fontdict={'fontweight':'bold'})\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0,3])\n",
    "    ax4.plot(phit, phit.index, 'k', lw=2, label='phit')\n",
    "    ax4.plot(phi, phi.index, 'r', lw=0.5, label='phin')\n",
    "    ax4.plot(phid, phid.index, 'g', lw=0.5, label='phid')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_ylabel('sample no')\n",
    "    ax4.invert_xaxis()\n",
    "    ax4.set_xlabel('Phi')\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "    ax4.legend(loc='best')\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[0,4], yticklabels=())\n",
    "    ax5.plot(umaa, umaa.index)\n",
    "    ax5.invert_yaxis()\n",
    "    ax5.set_xlabel('UMAA')\n",
    "    ax5.xaxis.tick_top()\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    \n",
    "    ax6 = fig.add_subplot(gs[0,5], yticklabels=())\n",
    "    ax6.plot(rhomaa, rhomaa.index)\n",
    "    ax6.invert_yaxis()\n",
    "    ax6.set_xlabel('RHOMAA')\n",
    "    ax6.xaxis.tick_top()\n",
    "    ax6.xaxis.set_label_position('top')\n",
    "    \n",
    "    return fig\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate Rhomaa & Umaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rhomaa_umaa(pe, dens, phi):\n",
    "    \"\"\"Plot log data using RHOMAA-UMAA crossplot for mineralogy\n",
    "    \n",
    "    Parameters:\n",
    "    pe (ndarray-like) : PE curve\n",
    "    dens (ndarray-like) : Density curve\n",
    "    phi (ndarray-like) : Neutron porosity curve\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # First, create the endmember points as a dataframe\n",
    "    index = ['Quartz', 'Calcite', 'Dolomite', 'Anhydrite', 'K-Feldspar']\n",
    "    data = {\n",
    "        'rhoma' : [2.65, 2.71, 2.87, 2.95, 2.54],\n",
    "        'uma' : [4.82, 13.79, 8.98, 14.99, 7.29],\n",
    "    }\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    \n",
    "    # Second, calculate umaa & rhomaa for the input logs\n",
    "    u = pe * dens\n",
    "    phid = (2.65 - dens)/(2.65-1.)\n",
    "    phit = (phid + phi)/2\n",
    "    #umaa = u / (1 - phit)\n",
    "    #umaa = (u - phit*0.5)/(1-phit)/2\n",
    "    rhomaa = (dens - phit*1.)/(1-phit) # assume brackish pore fluid rhof = 1.1 g/cc\n",
    "    umaa = (pe * rhomaa) / 2\n",
    "    \n",
    "    return rhomaa, umaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Vp versus Vs using well known rock physics templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vp_vs(x='DTC', y='DTS', df=None):\n",
    "    \"\"\"\n",
    "    Plots measured Vp & Vs against well-known rock physics trends\n",
    "    \n",
    "    Parameters:\n",
    "    x (pandas.Series) : Input DTC values, in us/ft\n",
    "    y (pandas.Series) : Input DTS values, in us/ft\n",
    "    df (pandas.DataFrame) : Input data frame containing x & y\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    vp_ft_s = 1e6 / df[x]\n",
    "    vp_m_s = vp_ft_s /  3.281\n",
    "    vp_km_s = vp_m_s / 1000\n",
    "    \n",
    "    vs_ft_s = 1e6 / df[y]\n",
    "    vs_m_s = vs_ft_s / 3.281\n",
    "    vs_km_s = vs_m_s / 1000\n",
    "    \n",
    "    xvp = np.arange(start=0, stop=8, step=0.1)\n",
    "    \n",
    "    Vs_castagna_ls = np.multiply(-0.05508, np.power(xvp, 2)) + np.multiply(1.0168, xvp) - 1.0305\n",
    "    Vs_castagna_dm = np.multiply(0.5832, xvp) - 0.07776\n",
    "    Vs_castagna_mudrock = np.multiply(0.8621, xvp) - 1.1724\n",
    "    Vs_castagna_ss = np.multiply(0.8042, xvp) - 0.8559\n",
    "    \n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(Vs_castagna_ls, xvp, '--b', lw=2, label='Castagna et al. (1993) Water-saturated Limestone')\n",
    "    ax.plot(Vs_castagna_dm, xvp, '--c', lw=2, label='Castagna et al. (1993) Water-saturated Dolomite')\n",
    "    ax.plot(Vs_castagna_mudrock, xvp, '--k', lw=2, label='Castagna et al. (1993) Mudrock line')\n",
    "    ax.plot(Vs_castagna_ss, xvp, '--r', lw=2, label='Castagna et al. (1993) Water-saturated sandstone')\n",
    "    im = ax.scatter(vs_km_s, vp_km_s, s=10, c=df.index, marker='.', cmap='inferno', alpha=0.8)\n",
    "    ax.set_xlabel('Vs (km/s)')\n",
    "    ax.set_ylabel('Vp (km/s)')\n",
    "    ax.set_title('Vp vs. Vs', fontsize=20, fontweight='bold')\n",
    "    ax.set_xlim([np.nanmin(vs_km_s), np.nanmax(vs_km_s)])\n",
    "    ax.set_ylim([np.nanmin(vp_km_s), np.nanmax(vp_km_s)])\n",
    "    ax.legend(loc='upper left')\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.ax.invert_yaxis()\n",
    "    cbar.set_label('sample no.', fontdict={'fontweight':'bold'})\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Training Real vs. Training Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_pred(y, y_pred, feat_name, split):\n",
    "    \"\"\"Function that plots y vs y_pred\n",
    "    \n",
    "    Parameters:\n",
    "    y (ndarray like) : real values\n",
    "    y_pred (ndarray like) : predicted values\n",
    "    feat_name (str) : Feature name\n",
    "    split (str) : Whether this is 'Train', 'Test', or 'Full Log'\n",
    "    \n",
    "    returns matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    fig.suptitle('Real {} vs. Predict'.format(split))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.plot(y.reset_index(drop=True, inplace=False), 'k', lw=2, label='y {}'.format(split))\n",
    "    ax1.plot(y_pred, 'r', lw=1, label='y_{}_pred'.format(split))\n",
    "    ax1.set_xlabel('sample no.')\n",
    "    ax1.set_ylabel(feat_name)\n",
    "    ax1.set_title('{} Real & Predict'.format(split))\n",
    "    ax1.legend(loc='best')\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.scatter(y, y_pred)\n",
    "    ax2.set_xlabel('{} Real {}'.format(feat_name, split))\n",
    "    ax2.set_ylabel('{} Predict {}'.format(feat_name, split))\n",
    "    ax2.set_title('{} {} Predict vs. Real'.format(feat_name, split))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to visually QC original vs. imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_imputed(df_original, df_imputed):\n",
    "    \"\"\"Plots original and imputed log curves for visual QC\n",
    "    \n",
    "    Parameters:\n",
    "    df_original (pandas.DataFrame) : Input data frame containing the well log curves, one log per column\n",
    "    \n",
    "    df_imputed (pandas.DataFrame) : Input data frame containing filtered log curves, one log per column\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the column names as a list, assumes col names are the same for each input data frame\n",
    "    curve_names = df_original.columns.tolist()\n",
    "    \n",
    "    # build the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Original vs Imputed Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(df_imputed[curve], df_imputed.index, color='r', lw=0.5, label='imputed')\n",
    "            ax.semilogx(df_original[curve], df_original.index, color='k', lw=2, label='original')\n",
    "        else:\n",
    "            ax.plot(df_imputed[curve], df_imputed.index, color='r', lw=0.5, label='imputed')\n",
    "            ax.plot(df_original[curve], df_original.index, color='k', lw=2, label='original')\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "        ax.legend(loc='upper left')\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to find consecutive NaN values and the starting index number\n",
    "[Consecutive NaN larger than threshold](https://stackoverflow.com/questions46007776/consecutive-nan-larger-than-threshold-in-pandas-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_nans(index, col, threshold):\n",
    "    \"\"\"Function returns the starting index of consecutive NaN's in an input column\n",
    "    Uses Divakar's answer in above link\n",
    "    \"\"\"\n",
    "    \n",
    "    thresh = threshold\n",
    "    \n",
    "    a = index.values\n",
    "    b = col.values\n",
    "    \n",
    "    idx0 = np.flatnonzero(np.r_[True, np.diff(np.isnan(b))!=0, True])\n",
    "    count = np.diff(idx0)\n",
    "    idx = idx0[:-1]\n",
    "    valid_mask = (count>=thresh) & np.isnan(b[idx])\n",
    "    out_idx = idx[valid_mask]\n",
    "    out_num = a[out_idx]\n",
    "    out_count = count[valid_mask]\n",
    "    out = range(int(out_num),int(out_num)+int(out_count)-1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"read-data\"></a>Read in Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1 = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"inspect-data\"></a>Inspect Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data are type float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples with value of -999.0 need to be replaced with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.replace(to_replace=-999.0, value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some suspicious values here that we will explore in the next step.\n",
    "\n",
    "1. CNC should range from 0.0 to 1.0\n",
    "1. GR should not have values less than 0.0\n",
    "1. PE should not have values less than 0.0\n",
    "1. ZDEN should not have values less than 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"eda\"></a>4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making any edits to values, plot the data for visual QC inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a [RHOMAA-UMAA crossplot](http://www.kgs.ku.edu/Publications/Bulletins/LA/11_crossplot.html) to see if we can differentiate lithologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_w1['PE'], df_w1['ZDEN'], df_w1['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely outliers are present!\n",
    "\n",
    "For fun, lets see what a Short-Time Fourier Transform (STFT) of the well logs look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_plot = plot_stft_well_curves(df_w1.interpolate(limit_area='inside'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit CNC to range between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['CNC'] < 0.0, ['CNC']] = np.nan\n",
    "df_w1.loc[df_w1['CNC'] > 1.0, ['CNC']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit GR to range between 0.0 and 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['GR'] < 0.0, ['GR']] = np.nan\n",
    "df_w1.loc[df_w1['GR'] > 300.0, ['GR']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit PE to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['PE'] < 0.0, ['PE']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit ZDEN to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['ZDEN'] < 0.0, ['ZDEN']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These value ranges are substantially more acceptable.  Re-plot the data and visually inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this appears to be better.  The PE final 10,000 values look suspect, though.\n",
    "\n",
    "Let's attempt filtering out the spikes remaining in the data.\n",
    "\n",
    "The filter first calculates an n-moving window median filtered version of the data and then eliminates values that fall outside of +/- n-standard deviations.\n",
    "\n",
    "It might make sense to first standardize the data using RobustScaler (uses interquartile range 25-75% of the data) and pass the standardized data to the filter.\n",
    "\n",
    "After filter, the data can be transformed back to its non-standardized domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "scaled_features = rs.fit_transform(df_w1.values)\n",
    "scaled_features_df = pd.DataFrame(data=scaled_features, index=df_w1.index, columns=df_w1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(scaled_features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(scaled_features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the scaled data to the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_scaled, df_outliers_scaled = filter_curves(scaled_features_df, 27, 2)\n",
    "\n",
    "# Backtransform the cleaned data & outliers to the unscaled domain\n",
    "clean_unscaled = rs.inverse_transform(df_clean_scaled.values)\n",
    "df_clean = pd.DataFrame(data=clean_unscaled, columns=df_w1.columns)\n",
    "outliers_unscaled = rs.inverse_transform(df_outliers_scaled.values)\n",
    "df_outliers = pd.DataFrame(data=outliers_unscaled, columns=df_w1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_fig = plot_well_curves(df_clean)\n",
    "#cleaned_fig = plot_well_curves(df_clean.fillna(method='bfill').fillna(method='ffill'))\n",
    "cleaned_fig = plot_well_curves(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "#cleaned_fig = plot_well_curves(df_clean.dropna(axis=0, inplace=False).interpolate(limit_area='inside'))\n",
    "#cleaned_fig = plot_well_curves(df_clean.dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qc_fig = qc_curve_filt(df_w1, df_clean)\n",
    "qc_fig = qc_curve_filt(df_w1, df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "#qc_fig = qc_curve_filt(df_w1, df_clean, df_outliers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC lithologies with RHOMAA-UMAA crossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_clean['PE'].interpolate(limit_area='inside'), df_clean['ZDEN'].interpolate(limit_area='inside'), df_clean['CNC'].interpolate(limit_area='inside'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the index number where the bad PE values begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['PE'] < 1].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering operation has done a good job of removing outliers while keeping the overall signal intact\n",
    "\n",
    "PE definitely has some bad values, as evident both on the log plot and the RHOMAA-UMAA transform crossplot.\n",
    "\n",
    "**CAL seems a little suspect, though.  It seems unusual for CAL to increase at the bottom of the hole**\n",
    "\n",
    "In fact, CAL and PE basically go bad at the same sample number, so it is likely they had a shared tool failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to proceed.\n",
    "\n",
    "A few observations:\n",
    "\n",
    "* CAL could be useful (variations in the wellbore are related to geomechanical properties), but it can also be rather stationary.\n",
    "* If we want to maximize the amount of data available for estimating DTC & DTS, then interpolation is necessary.\n",
    "* The last 10,000 samples in the PE curve appear to be bogus.  Maybe these can be estimated using the shallower section?\n",
    "* HRM (Medium Resisitivity) should be dropped.  Without knowing the depth of investigation, it is likely to be contaminated with well-bore fluids.  HRD (Deep Resisitivity) is the better resistivity curve to use.  Maybe we could calculate Rt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several predictors exhibit a degree of multicollinearity\n",
    "\n",
    "* CNC & GR, which also exhibits heteroscedasticity\n",
    "* CNC & ZDEN\n",
    "* HRM & HRD\n",
    "\n",
    "Feature ranking will be an important step\n",
    "\n",
    "PCA may also be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Correlation Coefficient Heatmap of Well Log Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* CAL has reasonably high positive correlation to both DTC & DTS, as well as CNC.\n",
    "* CNC has reasonably high positive correlation to CAL, GR, PE, as well as to DTC & DTS\n",
    "* GR has moderate positive correlation to DTC & DTS\n",
    "* HRD & HRM do not exhibit strong correlation with any other parameters than themselves, *but this could be due to the fact they are log-scale*\n",
    "* PE has moderately positive correlation with CNC, DTC, & DTS\n",
    "* ZDEN has a high negative correlation with CNC, DTC, & DTS, and mild negative correlation with CAL, GR, & PE\n",
    "\n",
    "The recommendation going forward will be to drop CAL & HRM from the set of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_clean.copy().drop(labels=['CAL','HRM'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"imputing\"></a> Imputing missing values\n",
    "\n",
    "Back to [top](#top)\n",
    "\n",
    "**It's experimental, but let's try using Scikit-learn's IterativeImputer, which is similar to R's MICE algorithm.**\n",
    "\n",
    "IterativeImputer attempts to estimate missing values in a round-robin where each feature is individually treated as a dependent response using the other features to predict that feature.\n",
    "\n",
    "We will test IterativeImputer using BaysianRidge (default estimator for IterativeImputer).\n",
    "\n",
    "[Sklearn IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)\n",
    "\n",
    "[Imputing missing values with variants of IterativeImputer](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py)\n",
    "\n",
    "[Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "\n",
    "[Missing Data and Using Sklearn's IterativeImputer](https://andymdc31.github.io/missing_data_and_using_sklearns_iterativeimputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's replace all bad values in PE starting at index 19939 to the end with NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['PE'].iloc[19939:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(df_subset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, transform the data using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "standardscaled_features = ss.fit_transform(df_subset.values)\n",
    "standardscaled_features_df = pd.DataFrame(data=standardscaled_features, index=df_subset.index, columns=df_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(standardscaled_features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a BayesianRidge for input to IterativeImputer (technically this is not necessary as it is the default estimator).  Next we'll try ExtraTrees after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BayesianRidge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an IterativeImputer for both estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_br = IterativeImputer(estimator=br, sample_posterior=True, initial_strategy='mean', min_value=0, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by fitting IterativeImputer using BayesianRidge estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_br.fit(standardscaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of iterations: {}'.format(imp_br.n_iter_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_br_transform = imp_br.transform(standardscaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtransform the imputed data to the unstandardized domain\n",
    "imp_br_transform_unscaled = ss.inverse_transform(imp_br_transform)\n",
    "df_subset_imp_br = pd.DataFrame(data=imp_br_transform_unscaled, columns=df_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_original_vs_imputed(df_subset, df_subset_imp_br)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BayesianRidge IterativeImputer**\n",
    "\n",
    "This is actually a cool result.  Overall, it appears that the IterativeImputer using a BayesianRidge estimator does a good job of estimating each feature.\n",
    "\n",
    "It is really obvious how it works, too.  The initial estimatation strategy is to take the mean of each feature.  You can see on each that the estimated log (in red) appears to have a baseline roughly in the middle which likely corresponds to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So.**\n",
    "\n",
    "We will use a function *consecutive_nans* to find the gaps we want to fill with imputed values.  *consecutive_nans* searches for consecutive NaN values in a column above a specified threshold and returns a range that starts at the beginning of the consecutive NaNs and ends at the last NaN.\n",
    "\n",
    "Once these values are spliced in, interpolate to fill any remaining *small* gaps in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_imputed = df_subset.copy()\n",
    "\n",
    "dtc_nans = consecutive_nans(df_subset.index, df_subset['DTC'], threshold=100)\n",
    "df_subset_imputed['DTC'].iloc[dtc_nans] = df_subset_imp_br['DTC'].iloc[dtc_nans].rolling(window=27, center=True).max().rolling(window=101, center=True).median()\n",
    "\n",
    "dts_nans = consecutive_nans(df_subset.index, df_subset['DTS'], threshold=100)\n",
    "df_subset_imputed['DTS'].iloc[dts_nans] = df_subset_imp_br['DTS'].iloc[dts_nans].rolling(window=27, center=True).max().rolling(window=101, center=True).median()\n",
    "\n",
    "pe_nans = consecutive_nans(df_subset.index, df_subset['PE'], threshold=1000)\n",
    "df_subset_imputed['PE'].iloc[pe_nans] = df_subset_imp_br['PE'].iloc[pe_nans].rolling(window=27, center=True).max().rolling(window=101, center=True).median()\n",
    "\n",
    "df_subset_imputed = df_subset_imputed.interpolate(limit_area='inside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(df_subset_imputed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_curve_filt(df_subset.interpolate(limit_area='inside'), df_subset_imputed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputed values for DTC & DTS look plausible.\n",
    "\n",
    "The imputed values for PE seem off trend and too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petro_plot(df_subset_imputed['GR'], df_subset_imputed['HRD'], df_subset_imputed['CNC'], df_subset_imputed['ZDEN'], df_subset_imputed['PE'], df_subset_imputed['DTC'], df_subset_imputed['DTS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have a continuous data set for each curve.**\n",
    "\n",
    "Visually, the curves are appealing and look like data that we might receive from a petrophysicist upon completion of log edit & QC.\n",
    "\n",
    "**A few thoughts and observations:**\n",
    "\n",
    "  1. The imputed values are noticeably smoother than the real values.  Will this affect model performance?\n",
    "  \n",
    "  1. DTS imputed values are not as stable as DTC.\n",
    "  \n",
    "  1. Overall, the data does not look overly-processed.  Hopefully enough variation is preserved at this point by using n-standard deviation from the median filters to build a robust model.\n",
    "    \n",
    "  1. The estimated section of PE seems too high in value?  Are these estimated values reliable?  Maybe a different method needs to be used for PE.\n",
    "    \n",
    "  1. Aside from PE, these curves are ready for model fitting.  If the model does not perform well, we can:\n",
    "    \n",
    "      a. Splice the filtered imputed sections back into the previous iteration of filtered curves and use these for model building,\n",
    "      \n",
    "      b. Use interpolated trend for DTC & DTS instead of imputed values, or\n",
    "       \n",
    "      c. Abandon the data-driven solution for imputing & estimating missing values for traditional physics-based rock property model approaches.\n",
    "      \n",
    "**Let's continue by comparing where we started the imputation process to where we finished:**\n",
    "\n",
    "1. Raw curves with minimal edits for obviously bad values *vs.* Median-Filtered Curves\n",
    "\n",
    "1. Median-Filtered Curves *vs.* Real data with imputed data spliced in + Additional Median-Filtering\n",
    "\n",
    "1. Real data with imputed data spliced in *vs.* Real data with imputed data spliced in + Additional Median-Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_curve_filt(df_w1.drop(labels=['CAL','HRM'], axis=1, inplace=False), df_subset)\n",
    "qc_curve_filt(df_subset, df_subset_imputed)\n",
    "qc_curve_filt(df_w1.drop(labels=['CAL','HRM'], axis=1, inplace=False), df_subset_imputed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset_imputed['PE'], df_subset_imputed['ZDEN'], df_subset_imputed['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputed PE values are very obvious here.  The RHOMAA-UMAA plot suggests this section is calcitic, but plots outside of the calcite endmember.  I'm not very trusting of these values at this point ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_subset_imputed.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare each feature to the response variables, DTC & DTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df_subset_imputed.columns.tolist()\n",
    "predictors = [x for x in all_features if x not in ['DTC', 'DTS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in predictors:\n",
    "    ax = compare_curves(df_subset_imputed[col], df_subset_imputed['DTC'], df_subset_imputed['DTS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, it seems prudent to make some plots comparing our final cleaned curves to established rock property trends as a QC of the imputation and filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use plot_vp_vs to plot the imputed data against well known lithology trends, first the *pre-imputation* data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vp_vs(x='DTC', y='DTS', df=df_subset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And then data *with* imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vp_vs(x='DTC', y='DTS', df=df_subset_imputed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputed values don't necessarily conform to the observed trends.\n",
    "\n",
    "**A few observations:**\n",
    "\n",
    "1. Limestone appears to be a significant lithology.\n",
    "\n",
    "1. Shale / Claystone also appear to be present throughout the well.\n",
    "\n",
    "1. Sands are present, but are difficult to distinguish from Limestone unless they are gas-charged.  Gas-charged sands fall below the water-saturated sandstone line.\n",
    "\n",
    "1. There may be a minor amount of dolomite present in the system.\n",
    "\n",
    "1. Vp & Vs trends are obviously dependent upon lithology.  Plotting all of the data together, there is obvious heteroscedasticity.  Linear models would not fare well directly predicting Vs from Vp.  However, by using other features which are sensitive to lithologic variations, we should be able to build a robust model to handle these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot RHOMAA-UMAA for only portion of well with missing DTC ...** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset.interpolate(limit_area='inside')['PE'].iloc[dtc_nans], df_subset.interpolate(limit_area='inside')['ZDEN'].iloc[dtc_nans], df_subset.interpolate(limit_area='inside')['CNC'].iloc[dtc_nans])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**... and for DTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset.interpolate(limit_area='inside')['PE'].iloc[dts_nans], df_subset.interpolate(limit_area='inside')['ZDEN'].iloc[dts_nans], df_subset.interpolate(limit_area='inside')['CNC'].iloc[dts_nans])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be interesting to try *k*-means clustering of RHOMAA-UMAA on the pre-imputed dataset\n",
    "\n",
    "RHOMAA-UMAA is used as an approach to cluster data by petrophysical responses.  Typically, this is achieved using a bimineral or multimineral fraction calculation.  Maybe KMeans can do it for us.\n",
    "\n",
    "First, we need to add calculate RHOMAA & UMAA over the portion of the well we have valid PE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_ru = df_subset.copy().iloc[:pe_nans[0]]\n",
    "df_subset_ru.drop(labels=['DTC','DTS'], axis=1, inplace=True)\n",
    "df_subset_ru.interpolate(limit_area='inside', inplace=True)\n",
    "df_subset_ru.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset_ru['PE'], df_subset_ru['ZDEN'], df_subset_ru['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_ru['RHOMAA'], df_subset_ru['UMAA'] = calculate_rhomaa_umaa(df_subset_ru['PE'], df_subset_ru['ZDEN'], df_subset_ru['CNC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a new StandardScaler including RHOMAA & UMAA, fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_ru = StandardScaler()\n",
    "standardscaled_features_ru = ss_ru.fit_transform(df_subset_ru.values)\n",
    "standardscaled_features_ru_df = pd.DataFrame(data=standardscaled_features_ru, index=df_subset_ru.index, columns=df_subset_ru.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the RHOMAA & UMAA columns for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardscaled_features_ru_df[['UMAA','RHOMAA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate & Fit Kmeans\n",
    "\n",
    "What value should we set for n_clusters?  We expect the lithology to be something like 1) sandstone, 2) shale, 3) limestone, 4) dolomite, & 5) other.  Let's try 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=random_state)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset_ru['PE'], df_subset_ru['ZDEN'], df_subset_ru['CNC'], color_var=kmeans.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_ru['clusters'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(df_subset_ru)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gameplan:**\n",
    "\n",
    "1. Estimate missing PE for samples beyond 20000 and splice in\n",
    "1. Estimate missing DTC & DTS using either Faust or Gardner and splice in\n",
    "1. Build a data-driven model for estimating DTC & DTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"fe\"></a> 6. Feature Engineering\n",
    "\n",
    "Back to [top](#top)\n",
    "\n",
    "Before proceeding to estimating DTC & DTS, if we want to use as much of the data as possible, we need to attempt to estimate the bad values for PE as well as missing DTC & DTS.\n",
    "\n",
    "1. [6a. PE Curve Estimation](#fe-pe)\n",
    "1. [6b. DTC Approximation](#fe-dtc)\n",
    "1. [6c. DTS Approximation](#fe-dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-pe\"></a> 6a. Feature Engineering - PE Curve Estimation\n",
    "\n",
    "Back to [top](#top)\n",
    "\n",
    "The goal is to estimate PE for the final 10,000 samples that appear to have a bad measurement.\n",
    "\n",
    "If we can robustly estimate the PE curve that appears to be usable, then we can splice in the estimated section for the bad data samples, keeping the good data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously observed that CNC & ZDEN were most correlated to PE, so take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_subset_interp[['CNC','PE','ZDEN']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth crossplotting PE and HRD using a semilog scale to see if there is any relation not immediately noticeable in linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sns.scatterplot(x='HRD', y='PE', data=df_subset_interp)\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify at which sample the values become constantly bad.  Maybe one way to do this is to look at the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp[df_subset_interp['PE'] < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick df filter operation suggests that the bad PE values start at index 19338."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.plot(df_subset_interp['PE'].index, np.gradient(df_subset_interp['PE']))\n",
    "ax1.scatter(19338, df_subset_interp['PE'].iloc[19338], c='k', s=100)\n",
    "ax1.set_xlabel('sample no.')\n",
    "ax1.set_ylabel('Gradient of PE')\n",
    "ax1.set_title('Gradient of PE vs. Sample No.')\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.plot(range(19200, 20000), np.gradient(df_subset_interp['PE'])[19200:20000])\n",
    "ax2.scatter(19338, df_subset_interp['PE'].iloc[19338], c='k', s=100)\n",
    "ax2.set_xlabel('sample no.')\n",
    "ax2.set_ylabel('Gradient of PE')\n",
    "ax2.set_title('Zoomed plot about PE Gradient Samples')\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "ax3.plot(np.gradient(np.gradient(df_subset_interp['PE'])))\n",
    "ax3.set_xlabel('sample no.')\n",
    "ax3.set_ylabel('Second derivative of PE')\n",
    "ax3.set_title('Second derivative of PE')\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "ax4.plot(range(19200, 20000), np.gradient(np.gradient(df_subset_interp['PE']))[19200:20000], 'k', label='Second derivative')\n",
    "ax4.plot(range(19200, 20000), np.gradient(df_subset_interp['PE'])[19200:20000], 'r', label='First derivative')\n",
    "ax4.scatter(19338, df_subset_interp['PE'].iloc[19338], s=100)\n",
    "ax4.set_xlabel('Sample no.')\n",
    "ax4.set_ylabel('Second Derivative of PE')\n",
    "ax4.set_title('Second Derivative of PE')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection of the first and second derivative confirm that index 19338 is the start of the offending data.\n",
    "\n",
    "Index Number 19338 is the point at which PE values appear to go consistently bad.  We can use all data up to this point for attempting to estimate PE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe = df_subset_interp[['CNC','HRD','ZDEN','PE']].iloc[0:19338].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the log of 'HRD' to put it in the same scale as CNC, ZDEN, & PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe['HRD'] = df_pe['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.rename(columns={'HRD':'log_HRD'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.log(df_pe['HRD']).plot()\n",
    "df_pe['log_HRD'].plot()\n",
    "df_pe['PE'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train-Test-Split\n",
    "1. Randomized Parameter Search + Cross Validation (Hyperparameter Tuning)\n",
    "1. Grid-Search + K-Fold Cross Validation (Hyperparameter Fine Tuning)\n",
    "1. Model Evaluation\n",
    "1. QC real vs. predicted PE\n",
    "1. Splice predicted data in for bad data, keeping good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_pe[['CNC', 'log_HRD', 'ZDEN']], df_pe['PE'], test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a default Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a random grid for RandomizedSearchCV to select initial hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(start=10, stop=100, num=10)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=5, verbose = 2, random_state=random_state, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Root Mean Squared Error for RandomForestRegressor with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_random.best_estimator_.predict(X_train)\n",
    "print('Training RMSE: {}'.format(mean_squared_error(y_train, y_train_pred, squared=False)))\n",
    "print('R2 Score: {}'.format(r2_score(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Root Mean Squared Error for RandomForestRegressor with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_random.best_estimator_.predict(X_test)\n",
    "print('Testing RMSE: {}'.format(mean_squared_error(y_test, y_test_pred, squared=False)))\n",
    "print('R2 Score: {}'.format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to a non-tuned default RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_default = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default RFR Training Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_default_pred = rf_default.predict(X_train)\n",
    "print('Default RF Training RMSE: {}'.format(mean_squared_error(y_train, y_train_default_pred, squared=False)))\n",
    "print('Default RF R2 Score :{}'.format(r2_score(y_train, y_train_default_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default RFR Testing Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_default_pred = rf_default.predict(X_test)\n",
    "print('Default RF Testing RMSE: {}'.format(mean_squared_error(y_test, y_test_default_pred, squared=False)))\n",
    "print('Default RF R2 Score: {}'.format(r2_score(y_test, y_test_default_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial hyperparameter tuning using RandomizedSearchCV has improved the scores, but let's now use GridSearchCV to attempt to tune for a little more performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40, 45, 50, 55, 60],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=700, stop=900, num=21)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV Train RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_gs_pred = grid_search.best_estimator_.predict(X_train)\n",
    "print('RF GridSearch Best Estimator RMSE: {}'.format(mean_squared_error(y_train, y_train_gs_pred, squared=False)))\n",
    "print('RF GridSearch Best Estimator R2: {}'.format(r2_score(y_train, y_train_gs_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_gs_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print('RF GridSearch Best Estimator Test RMSE: {}'.format(mean_squared_error(y_test, y_test_gs_pred, squared=False)))\n",
    "print('RF GridSearch Best Estimator Test R2: {}'.format(r2_score(y_test, y_test_gs_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator from gridsearch performed slightly worse on the training data but slightly better on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a new RandomForestRegressor using the parameters of the GridSearchCV best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Yellowbrick to evaluate model performance further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(rf_best)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training vs. Predict & Test vs. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(y_train, y_train_gs_pred, feat_name='PE', split='Train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(y_test, y_test_gs_pred, feat_name='PE', split='Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the entire curve using the model to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_whole_gs_pred = rf_best.predict(df_pe[['CNC', 'log_HRD', 'ZDEN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_pe['PE'], y_whole_gs_pred, feat_name='PE', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby for the clean section of the log!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the entire predicted log look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe_pred = pd.DataFrame()\n",
    "df_pe_pred['CNC'] = df_subset_interp['CNC']\n",
    "df_pe_pred['log_HRD'] = np.log(df_subset_interp['HRD'])\n",
    "df_pe_pred['ZDEN'] = df_subset_interp['ZDEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe_pred['PE_est'] = rf_best.predict(df_pe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['PE'], df_pe_pred['PE_est'], feat_name='PE', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have what appears to be a robust estimate of the missing PE data, we can splice it in to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['PE_est'] = df_subset_interp['PE']\n",
    "df_subset_interp['PE_est'].iloc[19338:] = df_pe_pred['PE_est'].iloc[19338:]\n",
    "df_subset_interp['log_HRD'] = df_subset_interp['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['PE'], df_subset_interp['PE_est'], feat_name='PE', split='Full Log with Splicing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check RHOMAA-UMAA crossplot with predicted PE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset_interp['PE_est'], df_subset_interp['ZDEN'], df_subset_interp['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_subset_interp.drop(labels='PE', axis=1, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_subset_interp.drop(labels=\"PE\", axis=1, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Corr Coef Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-dtc\"></a> 6b. Feature Engineering - DTC Approximation\n",
    "\n",
    "back to [Feature Engineering](#fe)\n",
    "\n",
    "back to [top](#top)\n",
    "\n",
    "Next, we will attempt to approximate the missing DTC values where previously we have used a linear interpolation method.  There are several ways this could be accomplished.\n",
    "\n",
    "1. Use [Gardner's Equation](#gardner) (1974) : $\\rho = 0.23 * (V_{p}^{0.25})$, which can be re-written as $V_{p} = \\sqrt[0.25]{\\frac{\\rho}{0.23}}$\n",
    "1. Use Faust's Relationship (1951): $V_{p} = \\alpha*(R*Z)^{\\frac{1}{6}}$ where,\n",
    "\n",
    "    $\\alpha \\equiv$ a depth-related constant ranging from 1000-3400,\n",
    "    \n",
    "    $R \\equiv$ Resistivity, and\n",
    "    \n",
    "    $Z \\equiv$ Depth in feet\n",
    "    \n",
    "    *Unfortunately, Faust cannot be used here because we don't know actual depth, only sample number*\n",
    "    \n",
    "1. Use a [model approach](#dtc-model) using another curve(s) which tracks DTC closely.  For example, CNC correlates very highly with DTC.\n",
    "\n",
    "Recall that $V_{p} = \\frac{1E6}{DTC}$, and $DTC = \\frac{1E6}{V_{p}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gardner\"></a> 6b.1 Gardner's Equation for estimating DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vp_gard = df_subset_interp['ZDEN'].apply(lambda x: np.power((x/0.23), 1/0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vp_raw = 1e6/df_subset_interp['DTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(Vp_raw, 'k', lw=2, label='Vp_raw')\n",
    "ax.plot(Vp_gard, 'r', lw=1, label='Vp_gard')\n",
    "ax.set_xlabel('Sample no.')\n",
    "ax.set_ylabel('Vp')\n",
    "ax.set_title('Vp vs. Vp from Density using Gardner\\'s Equation')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Root Mean Square Error of Gardner Equation prediction using ZDEN: {}'.format(evaluate_rmse(Vp_raw, Vp_gard)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"dtc-model\"></a> 6b.2 Model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,6))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax1b = ax1.twinx()\n",
    "ax1b.plot(df_subset_interp['CNC'], 'r', lw=1, label='CNC')\n",
    "ax1.set_xlabel('Sample no.')\n",
    "ax1.set_ylabel('DTC')\n",
    "ax1b.set_ylabel('CNC', color='red')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.scatter(df_subset_interp['CNC'], df_subset_interp['DTC'])\n",
    "ax2.set_xlabel('CNC')\n",
    "ax2.set_ylabel('DTC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_subset_interp['CNC']).reshape(-1,1), np.array(df_subset_interp['DTC']).reshape(-1,1), test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR coeff: {}'.format(lr.coef_))\n",
    "print('LR intercept: {}'.format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR training score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('LR test score :{}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_cnc = lr.predict(np.array(df_subset_interp['CNC']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_cnc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['DTC'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTC'], DTC_cnc, feat_name='DTC', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTC from CNC: {}'.format(evaluate_rmse(df_subset_interp['DTC'], DTC_cnc[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This result is significantly more accurate compared to using the Gardner equation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(df_subset_interp['DTC'], 'k', lw=3, label='DTC_raw')\n",
    "ax1.plot((1e6/Vp_gard), 'b', lw=1, label='DTC_gard')\n",
    "ax1.plot((DTC_cnc+rng.normal(0,1,DTC_cnc.shape)), '--g', lw=1, label='DTC_cnc+random noise')\n",
    "ax1.plot(DTC_cnc, 'r', lw=1, label='DTC_cnc')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_xlabel('sample no.')\n",
    "ax1.set_ylabel('DTC')\n",
    "ax1.set_title('Comparing raw DTC vs. two prediction methods')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression for DTC on CNC is a better estimator and should be used for the missing section.  Adding in some random noise is probably a good idea so it is not directly related to CNC**\n",
    "\n",
    "*Alternatively, each could be used as as separate final models... One could be interpolated values, One could be DTC_gard, and One could be DTC_cnc*\n",
    "\n",
    "**Now we need to splice in the DTC_cnc estimate for the zone that is interpolated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax.plot(np.round(np.gradient(df_subset_interp['DTC']), 1), 'r', lw=1, label='gradient')\n",
    "ax.plot(df_subset_interp['DTC'].diff(), 'b', lw=0.25, label='diff')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('DTC')\n",
    "ax.set_title('Gradient of DTC')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp[np.abs(np.round(np.gradient(df_subset_interp['DTC']), 1)) == 0 ].loc[8441:12551,'DTC'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax.plot(df_subset_interp['DTC'].iloc[8441:12551], 'r', lw=2, label='DTC interp')\n",
    "ax.plot(np.round(np.gradient(df_subset_interp['DTC']),1), 'r', lw=1, label='gradient')\n",
    "ax.scatter(df_subset_interp[np.abs(np.round(np.gradient(df_subset_interp['DTC']), 1)) == 0].loc[8441:12551,'DTC'].index, np.zeros(4108), s=20, c='b', marker='.', label='interpolated data')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTC')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is probably a more elegant way to find the index range of the interpolated data, but for now this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['DTC_spliced'] = df_subset_interp['DTC']\n",
    "DTC_cnc_awgn = DTC_cnc + rng.normal(0,1,DTC_cnc.shape)\n",
    "df_subset_interp['DTC_spliced'].iloc[8441:12551] = DTC_cnc_awgn[8441:12551,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTC'], df_subset_interp['DTC_spliced'], feat_name='DTC', split='Full Log with Splicing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(df_subset_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTC_spliced looks like a plausible estimation for the missing data section.  Notice the slow velocity layer just above sample number 10,000.  The same slow down is seen on the DTS log.  Encouraging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petro_plot(df_subset_interp['GR'], df_subset_interp['HRD'], df_subset_interp['CNC'], df_subset_interp['ZDEN'], df_subset_interp['PE_est'], df_subset_interp['DTC_spliced'], df_subset_interp['DTS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-dts\"></a> 6c. Feature Engineering - DTS Approximation\n",
    "\n",
    "back to [Feature Engineering](#fe)\n",
    "\n",
    "back to [top](#top)\n",
    "\n",
    "Finally, there is a missing section of DTS that needs to be approximated.\n",
    "\n",
    "1. For self-consistency with DTC estimation, first build a [Linear Regression](#lr-dts) for DTS on to CNC.\n",
    "1. <a name=\"empirical-dts\"></a>Empirical Models\n",
    "    * [Castagna *et al.* (1993)](#castagna-1993): \n",
    "    ** Water Saturated Sands: $V_{s} (km/s) = 0.804V_{p} - 0.856$\n",
    "    ** Mudrock line: $V_{s} (km/s) = 0.862V_{p} - 1.172$\n",
    "    * [Brocher (2005)](#brocher-2005): $V_{s} (km/s) = 0.7858 - 1.2344V_{p} + 0.7949V_{p}^2 - 0.1238V_{p}^3 + 0.0064V_{p}^4$\n",
    "    * Castanga *et al.* (1985): \n",
    "    ** Shaley sands (Frio formation): $V_{s} (km/s) = 3.89 - 7.07{phi} - 2.04C$, (km/sec) where ${phi}$ is porosity & $C$ is clay fraction\n",
    "    ** Mudrock: $V_{s} (km/s) = 0.862V_{p} - 1.172$\n",
    "    \n",
    "    \n",
    "For more on empirical models for Vp, Vs, & more, see __The Rock Physics Handbook, 2e__ by Mavko *et al.* (2009).  It is always a handy reference in times of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced'])/(1e6/df_subset_interp['DTS']), 'k')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('Vp/Vs ratio')\n",
    "ax.set_title('Vp/Vs ratio', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vp/Vs ratio is anomalously high in the \"shallow\" section until about sample number 10,000 at which it averages about 1.7 - 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare CNC & DTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=2, label='DTS')\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(df_subset_interp['CNC'], 'r', lw=1, label='CNC')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('DTS')\n",
    "ax1.set_ylabel('CNC', fontdict={'color':'r'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a <a name=\"lr-dts\"></a>Linear Regression model to predict DTS from CNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_subset_interp['CNC']).reshape(-1,1), np.array(df_subset_interp['DTS']).reshape(-1,1), test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR coeff: {}'.format(lr.coef_))\n",
    "print('LR intercept: {}'.format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR training score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('LR test score :{}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTS_cnc = lr.predict(np.array(df_subset_interp['CNC']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTS'], DTS_cnc, feat_name='DTS', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from CNC: {}'.format(evaluate_rmse(df_subset_interp['DTS'], DTS_cnc[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an underwhelming result.  Whereas CNC was a relatively robust predictorfor DTC, the relationship does not seem to hold as well for DTS.\n",
    "\n",
    "Instead, there are several well-known [empirical models](#empirical-dts) which we can test to see if we can produce a better estimate of Vs for the interpolated section.\n",
    "\n",
    "Let's first test the well-known and widely applied <a name=\"castagna-1993\"></a>Castana *et al.* (1993):\n",
    "\n",
    "* For water saturated sandstones: $V_{s} (km/s) = 0.804V_{p} - 0.856$, and\n",
    "\n",
    "* The famous \"Mudrock\" line: $V_{s} (km/s) = 0.862V_{p} - 1.172$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_c93ss = 1000*(0.804*((1e6/df_subset_interp['DTC_spliced']/3.281)/1000) - 0.856)\n",
    "Vs_c93mr = 1000*(0.862*((1e6/df_subset_interp['DTC_spliced']/3.281)/1000) - 1.172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS')\n",
    "ax.plot(1e6/(3.281*Vs_c93ss), 'r', lw=1, label='Vs Castagna et al. (1993) Sandstone')\n",
    "ax.plot(1e6/(3.281*Vs_c93mr), 'b', lw=1, label='Vs Castagna et al. (1993) Mudrock')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('Vs (m/s)')\n",
    "ax.set_title('Comparison of DTS_raw & Castagna et al. (1993)')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Castagna et al. 1993 sandstone: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93ss))))\n",
    "print('RMSE predicting DTS from Castagna et al. 1993 mudrock: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93mr))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On the whole, this is not a drastically different from the linear regression of DTS onto CNC, but there are several reasons why.  The lithology is not simply all wet sandstone or all wet mudrock.  Using the wet sandstone line performs slightly worse than the CNC-model while the mudrock line performs slighty better.\n",
    "\n",
    "* To improve the model fit, we would need to know about the lithology.  \n",
    "\n",
    "* First, let's take a look at the Vp/Vs ratio using these two estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced'])/(1e6/df_subset_interp['DTS']), 'k', lw=3, label='Vp/Vs raw')\n",
    "ax.plot(df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced']/3.281)/Vs_c93ss, 'r', lw=1, label='Vp/Vs C93SS')\n",
    "ax.plot(df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced']/3.281)/Vs_c93mr, 'b', lw=1, label='Vp/Vs C93MR')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('Vp/Vs ratio')\n",
    "ax.set_title('Vp/Vs ratio', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Castagna *et al.* (1993) also has relations for limestones.  As this is the Volve field in the Norwegian North Sea, that is entirely a possibility:\n",
    "\n",
    "$V_{s} (km/s) = -0.055V_{p}^2 + 1.017V_{p} - 1.031$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_c93ls = 1000*(-0.055*np.power(1e6/df_subset_interp['DTC_spliced']/3.281/1000, 2) + 1.017*(1e6/df_subset_interp['DTC_spliced']/3.281/1000) - 1.031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS raw')\n",
    "ax.plot(1e6/(3.281*Vs_c93ss), 'r', lw=1, label='DTS C93 Sandstone')\n",
    "ax.plot(1e6/(3.281*Vs_c93mr), 'b', lw=1, label='DTS C93 Mudrock')\n",
    "ax.plot(1e6/(3.218*Vs_c93ls), 'g', lw=1, label='DTS C93 Limestone')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTS')\n",
    "ax.set_title('Comparison of DTS with Castagna et al. (1993) Mudrock & Limestone relation')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Castagna et al. 1993 limestone: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93ls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... while the overall RMSE of the limestone trend is about the same as the sandstone, there are some zones along the log that this is clearly a better fit, suggesting a substantial proportion of limestone\n",
    "\n",
    "Now, lets try <a name=\"brocher-2005\"></a>Brocher's (2005) relation for Vs from Vp:\n",
    "\n",
    "$V_{s} (km/s) = 0.7858 - 1.2344V_{p} + 0.7949V_{p}^2 - 0.1238V_{p}^3 + 0.0064V_{p}^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease, let's make a variable for Vp\n",
    "Vp_mps = 1e6/df_subset_interp['DTC_spliced']/3.281\n",
    "Vp_kmps = Vp_mps / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_brocher = 0.7858 - 1.2344*Vp_kmps + 0.7949*np.power(Vp_kmps, 2) - 0.1238*np.power(Vp_kmps, 3) + 0.0064*np.power(Vp_kmps, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS raw')\n",
    "ax.plot(1e6/(Vs_brocher*1000*3.281), 'r', lw=1, label='DTS Brocher 2005')\n",
    "ax.plot(1e6/(Vs_c93mr*3.281), '--b', lw=1, label='DTS Castagna et al. (1993) mudrock')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTS')\n",
    "ax.set_title('Comparison of Castagna et al. (1993) Mudrock and Brocher (2005)')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Brocher (2005): {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(Vs_brocher*1000*3.281))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brocher (2005) is almost identical to Castagna *et al.* (1993) Mudrock line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdda",
   "language": "python",
   "name": "pdda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
