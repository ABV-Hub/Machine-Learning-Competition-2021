{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"top\"></a>SWPLA PDDA 2020 Synthetic Sonic Log Generation Contest\n",
    "\n",
    "**Author:** Ben Dowdell\n",
    "\n",
    "**Date:** March XX, 2020\n",
    "\n",
    "**Purpose:** To minimize RMSE in predicting Sonic logs (compressional & shear) from a suite of standard well logs\n",
    "\n",
    "**v2 notebook goes with 'dowdell_submission_v2.csv'**\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [1. Initial Set-Up](#initial-setup)\n",
    "** [1a. Standard Imports](#standard-imports)\n",
    "** [1b. Sklearn Imports](#sklearn-imports)\n",
    "** [1c. Helper Function Definitions](#helper-funcs)\n",
    "* [2. Read Data](#read-data)\n",
    "* [3. Inspect Data](#inspect-data)\n",
    "* [4. EDA](#eda)\n",
    "* [5. Baseline Model](#baseline)\n",
    "* [6. DTC & DTS Model Train/Test](#model)\n",
    "* [7. Blind Test QC & Blind Test Prediction Output](#blind-test)\n",
    "* [8. Summary](#summary)\n",
    "\n",
    "### Data Decription\n",
    "#### Files\n",
    "#### train.csv\n",
    "All the values equals to -999 are marked as missing values.\n",
    "- CAL - Caliper, unit in Inch,  \n",
    "- CNC - Neutron, unit in dec \n",
    "- GR - Gamma Ray, unit in API\n",
    "- HRD - Deep Resisitivity, unit in Ohm per meter,\n",
    "- HRM - Medium Resistivity, unit in Ohm per meter,\n",
    "- PE - Photo-electric Factor, unit in Barn,\n",
    "- ZDEN - Density, unit in Gram per cubit meter, \n",
    "- DTC - Compressional Travel-time, unit in nanosecond per foot,\n",
    "- DTS - Shear Travel-time, unit in nanosecond per foot,\n",
    "\n",
    "\n",
    "#### test.csv\n",
    "The test data has all features that you used in the train dataset, except the two sonic curves DTC and DTS.\n",
    "\n",
    "####  sample_submission.csv\n",
    "A valid sample submission.\n",
    "<p><font style=\"\">\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"initial-setup\"></a>1. Initial Set-Up\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"standard-imports\"></a> 1a. Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from pprint import pprint\n",
    "\n",
    "random_state = 42\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sklearn-imports\"></a>1b. Sklearn Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing utilities\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel, RFE, f_regression\n",
    "\n",
    "# Model k-fold Cross Validation score for generalization evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering Models\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Yellowbrick tools for model analysis\n",
    "from yellowbrick.features import Rank1D, Rank2D\n",
    "from yellowbrick.features import PCA as PCA_yb\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "from yellowbrick.model_selection import FeatureImportances, ValidationCurve\n",
    "from yellowbrick.regressor import ResidualsPlot, PredictionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"helper-funcs\"></a>1c. Helper Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot data histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distributions(df):\n",
    "    \"\"\"Plot histograms of each curve\n",
    "    \n",
    "    Paramters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    flag = 0\n",
    "    n_subplots = len(df.columns.to_list())\n",
    "    if n_subplots % 2 == 0:\n",
    "        n_rows = 2\n",
    "        n_cols = n_subplots // n_rows\n",
    "    elif n_subplots % 3 == 0:\n",
    "        n_rows = 3\n",
    "        n_cols = n_subplots // n_rows\n",
    "    else:\n",
    "        n_cols = n_subplots // 2\n",
    "        n_rows = n_subplots // n_cols\n",
    "        n_rows += n_subplots % n_cols\n",
    "        n_blank = (n_rows * n_cols) - n_subplots\n",
    "        flag = 1\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12,8))\n",
    "    if flag == 1:\n",
    "        axes[-1,-n_blank].axis('off')\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df[col])\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function plot data CDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_cdf(df):\n",
    "    \"\"\"Plot cumulative distribution function of each curve\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,8))\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df[col], density=True, cumulative=-1)\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot well curves in a normal log display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_well_curves(data):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : Input data frame containing the well log curves, one per column\n",
    "    \n",
    "    curve_names (list) : A list containing the column name of each well log curve in the input data frame\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # get the column names as a list\n",
    "    curve_names = data.columns.tolist()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data[curve], data.index, color='k')\n",
    "        else:\n",
    "            ax.plot(data[curve], data.index, color='k')\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS'] or 'DT' in curve:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "    ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to remove outliers from log curves using standard deviation from median value in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_curves(data, window, n_std):\n",
    "    \"\"\"\n",
    "    Takes a data frame containing well log curves and filters outliers based on n-standard deviations\n",
    "    from a median filtered version of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : The input data frame containing data to be filtered.  Each column is a well log curve.\n",
    "    \n",
    "    window (int) : The size of the window to use in creating a median filtered curve (recommend 33)\n",
    "    \n",
    "    n_std (int) : The number of +/- standard deviations to use in considering outliers (recommend 2)\n",
    "    \n",
    "    Returns:\n",
    "    df_clean (pandas.DataFrame) : The filtered well curves\n",
    "    df_outliers (pandas.DataFrame) : The outlier data points removed by the filtering operation\n",
    "    \"\"\"\n",
    "    # create a copy of the original data\n",
    "    df_copy = data.copy()\n",
    "    \n",
    "    # create a data frame containing median-filtered version of the data\n",
    "    df_medfilter = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_medfilter[col] = df_copy[col].rolling(window, min_periods=1, center=True).median()\n",
    "        \n",
    "    # create a data frame containing standard deviation of the data\n",
    "    df_stddev = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_stddev[col] = df_medfilter[col].rolling(window, min_periods=1, center=True).std()\n",
    "        \n",
    "    # create a data frame containing the cleaned version of the data using standard deviation from the median filtered data\n",
    "    # and create a data frame containing the removed outliers\n",
    "    df_clean = pd.DataFrame()\n",
    "    df_outliers = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        upper = df_medfilter[col] + df_stddev[col]*n_std\n",
    "        lower = df_medfilter[col] - df_stddev[col]*n_std\n",
    "        df_clean[col] = df_copy[col].where((df_copy[col] <= upper) & (df_copy[col] >= lower))\n",
    "        #df_clean[col] = df_clean[col].interpolate(limit_area='inside')\n",
    "        df_outliers[col] = df_copy[col].where(df_clean[col] != df_copy[col])\n",
    "    \n",
    "    return df_clean, df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare raw versus filtered logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_curve_filt(data_raw, data_cleaned, *args):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data_raw (pandas.DataFrame) : Input data frame containing the well log curves, one log per column\n",
    "    \n",
    "    data_cleaned (pandas.DataFrame) : Input data frame containing filtered log curves, one log per column\n",
    "    \n",
    "    *args (pandas.DataFrame) : Optional data frame containing outliers\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the column names as a list, assumes col names are the same for each input data frame\n",
    "    curve_names = data_raw.columns.tolist()\n",
    "    \n",
    "    # build the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.semilogx(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.semilogx(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        else:\n",
    "            ax.plot(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.plot(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.plot(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare individual predictors to DTC & DTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_curves(predictor, dtc, dts):\n",
    "    \"\"\"\n",
    "    Takes a predictor curve and plots it against both response variables, DTC & DTS\n",
    "    \n",
    "    Parameters:\n",
    "    predictor (pandas.core.series.Series) : Independent Variable to compare, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dtc (pandas.core.series.Series) : DTC curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dts (pandas.core.series.Series) : DTS curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,10), linewidth=5, edgecolor='k')\n",
    "    name = predictor.name\n",
    "    fig.suptitle('{} curve comparison to DTC & DTS'.format(name), fontsize=20)\n",
    "    \n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    if name == 'HRD':\n",
    "        ax1.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax1.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax1c = ax1.twinx()\n",
    "    ax1c.plot(dtc.index, dtc.values, 'r', lw=2)\n",
    "    ax1c.invert_yaxis()\n",
    "    ax1.set_ylabel(name)\n",
    "    ax1c.set_ylabel('DTC', fontdict={'color':'r'})\n",
    "    ax1.grid(False)\n",
    "    ax1c.grid(False)\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2 = sns.scatterplot(x=predictor, y=dtc)\n",
    "    if name == 'HRD':\n",
    "        ax2.set_xscale('log')\n",
    "    ax2.set_xlabel(name)\n",
    "    ax2.set_ylabel('DTC')\n",
    "    ax2.grid(False)\n",
    "    \n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    if name == 'HRD':\n",
    "        ax3.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax3.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax3c = ax3.twinx()\n",
    "    ax3c.plot(dts.index, dts.values, 'r', lw=2)\n",
    "    ax3c.invert_yaxis()\n",
    "    ax3.set_ylabel(name)\n",
    "    ax3c.set_ylabel('DTS', fontdict={'color':'r'})\n",
    "    ax3.grid(False)\n",
    "    ax3c.grid(False)\n",
    "    \n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    ax4 = sns.scatterplot(x=predictor, y=dts)\n",
    "    if name == 'HRD':\n",
    "        ax4.set_xscale('log')\n",
    "    ax4.set_xlabel(name)\n",
    "    ax4.set_ylabel('DTS')\n",
    "    ax4.grid(False)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate Root Mean Squared Error (RMSE), the primary evaluation metric for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(real, predicted):\n",
    "    \"\"\"Calculates the Root Mean Square Error\n",
    "     \n",
    "     Parameters:\n",
    "     real (ndarray) : actual values as a numpy array\n",
    "     predicted (ndarray) : predicted values as a numpy array\n",
    "     \n",
    "     Returns: RMSE Accuracy as a float\n",
    "     \n",
    "    \"\"\"\n",
    "    mse = np.square(np.subtract(real, predicted)).mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot logs in a typical petrophysical layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def petro_plot(gr, resd, neut, den, pe, dtc, dts):\n",
    "    \"\"\"\n",
    "    Produces a typical petrophysical plot\n",
    "    \n",
    "    Parameters:\n",
    "    gr (pandas.Series) : Gamma Ray curve as a pandas series\n",
    "    resd (pandas.Series) : Deep Resistivity curve as a pandas series\n",
    "    neut (pandas.Series) : Neutron Porosity curve as a pandas series\n",
    "    den (pandas.Series) : Density curve as a pandas series\n",
    "    pe (pandas.Series) : PE curve as a pandas series\n",
    "    dtc (pandas.Series) : Compressional sonic curve as a pandas series\n",
    "    dts (pandas.Series) : Shear sonic curve as a pandas series\n",
    "    \n",
    "    Returns : matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "    # GR\n",
    "    ax1 = fig.add_subplot(1,5,1)\n",
    "    ax1.plot(gr, gr.index, 'k', lw=1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('GR')\n",
    "    ax1.set_ylabel('Sample no.')\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.set_xlim([0, 300])\n",
    "    ax1.set_xticks([0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300])\n",
    "    ax1.xaxis.set_label_position('top')\n",
    "\n",
    "    # RD\n",
    "    ax2 = fig.add_subplot(1,5,2)\n",
    "    ax2.semilogx(resd, resd.index, 'b', lw=1)\n",
    "    ax2.set_xlabel('HRD')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.xaxis.tick_top()\n",
    "    ax2.xaxis.set_label_position('top')\n",
    "\n",
    "    # N-D\n",
    "    ax3 = fig.add_subplot(1,5,3)\n",
    "    ax3.plot(den, den.index, 'r', lw=1)\n",
    "    ax3.xaxis.set_label_position('top')\n",
    "    ax3.set_xlabel('ZDEN', labelpad=10, fontdict={'color':'r'})\n",
    "    ax3.set_xlim(1.65, 2.65)\n",
    "    ax3.invert_yaxis()\n",
    "    ax3b = ax3.twiny()\n",
    "    ax3b.plot(neut, neut.index, 'g', lw=1)\n",
    "    ax3b.invert_xaxis()\n",
    "    ax3b.set_xlabel('CNC', fontdict={'color':'g'})\n",
    "    ax3b.set_xlim(0.6, 0.0)\n",
    "    ax3b.xaxis.tick_top()\n",
    "    ax3.xaxis.tick_top()\n",
    "    ax3.set_xticks([1.65, 1.85, 2.05, 2.25, 2.45, 2.65])\n",
    "    ax3.tick_params(axis='x', pad=35, top=True)\n",
    "    ax3b.xaxis.set_label_position('top')\n",
    "    ax3b.set_xticks([0.6, 0.48, 0.36, 0.24, 0.12, 0.0])\n",
    "\n",
    "    # PE\n",
    "    ax4 = fig.add_subplot(1,5,4)\n",
    "    ax4.plot(pe, pe.index, 'k', lw=1)\n",
    "    ax4.set_xlabel('PE')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "\n",
    "    # Sonic\n",
    "    ax5 = fig.add_subplot(1,5,5)\n",
    "    ax5.plot(dtc, dtc.index, 'b', lw=1)\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    ax5.set_xlabel('DTC', labelpad=10, fontdict={'color':'b'})\n",
    "    ax5.set_xlim(50, 170)\n",
    "    ax5.invert_xaxis()\n",
    "    ax5.invert_yaxis()\n",
    "    ax5b = ax5.twiny()\n",
    "    ax5b.plot(dts, dts.index, 'r', lw=1)\n",
    "    ax5b.invert_xaxis()\n",
    "    ax5b.set_xlabel('DTS', fontdict={'color':'r'})\n",
    "    ax5b.set_xlim(340, 100)\n",
    "    ax5b.xaxis.tick_top()\n",
    "    ax5.xaxis.tick_top()\n",
    "    #ax5.set_xticks([170, 150, 130, 110, 90, 70, 50])\n",
    "    ax5.set_xticks([230, 200, 170, 140, 110, 80, 50])\n",
    "    ax5.tick_params(axis='x', pad=35, top=True)\n",
    "    ax5b.xaxis.set_label_position('top')\n",
    "    #ax5b.set_xticks([340, 300, 260, 220, 180, 140, 100])\n",
    "    ax5b.set_xticks([460, 400, 340, 280, 220, 160, 100])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot RHOMAA-UMAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rhomaa_umaa(pe, dens, phi):\n",
    "    \"\"\"Plot log data using RHOMAA-UMAA crossplot for mineralogy\n",
    "    \n",
    "    Parameters:\n",
    "    pe (ndarray-like) : PE curve\n",
    "    dens (ndarray-like) : Density curve\n",
    "    phi (ndarray-like) : Neutron porosity curve\n",
    "    color_var (optional) : Default 'None' will use index as color.  Option to pass in labels such as kmeans.labels_\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # First, create the endmember points as a dataframe\n",
    "    index = ['Quartz', 'Calcite', 'Dolomite', 'Anhydrite', 'K-Feldspar']\n",
    "    data = {\n",
    "        'rhoma' : [2.65, 2.71, 2.87, 2.95, 2.54],\n",
    "        'uma' : [4.82, 13.79, 8.98, 14.99, 7.29],\n",
    "    }\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    \n",
    "    # Second, calculate umaa & rhomaa for the input logs\n",
    "    u = pe * dens\n",
    "    phid = (2.65 - dens)/(2.65-1.)\n",
    "    phit = (phid + phi)/2\n",
    "    #umaa = u / (1 - phit)\n",
    "    #umaa = (u - phit*0.5)/(1-phit)/2\n",
    "    rhomaa = (dens - phit*1.)/(1-phit) # assume brackish pore fluid rhof = 1.1 g/cc\n",
    "    umaa = (pe * rhomaa) / 2\n",
    "    \n",
    "    # Finally, create and return figure\n",
    "    fig = plt.figure(figsize=(28,10))\n",
    "    \n",
    "    gs = fig.add_gridspec(1, 6)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0,:-3])\n",
    "    im = ax1.scatter(umaa, rhomaa, s=10, c=umaa.index, marker='.', cmap='inferno', alpha=0.8)\n",
    "    ax1.scatter(df['uma'], df['rhoma'], c='r', marker='D')\n",
    "    for i, txt in enumerate(df.index.tolist()):\n",
    "        ax1.annotate(txt, (df['uma'].iloc[i]+0.03, df['rhoma'].iloc[i]-0.02), weight='bold')\n",
    "    ax1.plot(df['uma'].loc[['Quartz','Calcite']], df['rhoma'].loc[['Quartz', 'Calcite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Quartz', 'Dolomite']], df['rhoma'].loc[['Quartz', 'Dolomite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Dolomite','Calcite']], df['rhoma'].loc[['Dolomite','Calcite']], 'k')\n",
    "    ax1.set_xlabel('UMAA (barns/cc)')\n",
    "    #ax1.set_xlim(2,16)\n",
    "    ax1.set_ylabel('RHOMAA (g/cc)')\n",
    "    #ax1.set_ylim(2.3, 3.1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title('RHOMAA-UMAA Crossplot', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "    cbar = fig.colorbar(im, ax=ax1)\n",
    "    cbar.ax.invert_yaxis()\n",
    "    cbar.set_label('sample no', fontdict={'fontweight':'bold'})\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0,3])\n",
    "    ax4.plot(phit, phit.index, 'k', lw=2, label='phit')\n",
    "    ax4.plot(phi, phi.index, 'r', lw=0.5, label='phin')\n",
    "    ax4.plot(phid, phid.index, 'g', lw=0.5, label='phid')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_ylabel('sample no')\n",
    "    ax4.invert_xaxis()\n",
    "    ax4.set_xlabel('Phi')\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "    ax4.legend(loc='best')\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[0,4], yticklabels=())\n",
    "    ax5.plot(umaa, umaa.index)\n",
    "    ax5.invert_yaxis()\n",
    "    ax5.set_xlabel('UMAA')\n",
    "    ax5.xaxis.tick_top()\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    \n",
    "    ax6 = fig.add_subplot(gs[0,5], yticklabels=())\n",
    "    ax6.plot(rhomaa, rhomaa.index)\n",
    "    ax6.invert_yaxis()\n",
    "    ax6.set_xlabel('RHOMAA')\n",
    "    ax6.xaxis.tick_top()\n",
    "    ax6.xaxis.set_label_position('top')\n",
    "    \n",
    "    return fig, rhomaa, umaa\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Vp versus Vs using well known rock physics templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vp_vs(x='DTC', y='DTS', color='index', df=None):\n",
    "    \"\"\"\n",
    "    Plots measured Vp & Vs against well-known rock physics trends\n",
    "    \n",
    "    Parameters:\n",
    "    x (pandas.Series) : Input DTC values, in us/ft\n",
    "    y (pandas.Series) : Input DTS values, in us/ft\n",
    "    color (str) : Input variable for coloring scatter plot, acceptable values are 'index' and df column names\n",
    "    df (pandas.DataFrame) : Input data frame containing x & y\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    vp_ft_s = 1e6 / df[x]\n",
    "    vp_m_s = vp_ft_s /  3.281\n",
    "    vp_km_s = vp_m_s / 1000\n",
    "    \n",
    "    vs_ft_s = 1e6 / df[y]\n",
    "    vs_m_s = vs_ft_s / 3.281\n",
    "    vs_km_s = vs_m_s / 1000\n",
    "    \n",
    "    xvp = np.arange(start=0, stop=8, step=0.1)\n",
    "    \n",
    "    df_keys = df.columns.to_list()\n",
    "    color_mapping = dict()\n",
    "    color_mapping['index'] = df.index\n",
    "    \n",
    "    for key in df_keys:\n",
    "        if key not in color_mapping:\n",
    "            color_mapping[key] = df[key]\n",
    "    \n",
    "    Vs_castagna_ls = np.multiply(-0.05508, np.power(xvp, 2)) + np.multiply(1.0168, xvp) - 1.0305\n",
    "    Vs_castagna_dm = np.multiply(0.5832, xvp) - 0.07776\n",
    "    Vs_castagna_mudrock = np.multiply(0.8621, xvp) - 1.1724\n",
    "    Vs_castagna_ss = np.multiply(0.8042, xvp) - 0.8559\n",
    "    \n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(Vs_castagna_ls, xvp, '--b', lw=2, label='Castagna et al. (1993) Water-saturated Limestone')\n",
    "    ax.plot(Vs_castagna_dm, xvp, '--c', lw=2, label='Castagna et al. (1993) Water-saturated Dolomite')\n",
    "    ax.plot(Vs_castagna_mudrock, xvp, '--k', lw=2, label='Castagna et al. (1993) Mudrock line')\n",
    "    ax.plot(Vs_castagna_ss, xvp, '--r', lw=2, label='Castagna et al. (1993) Water-saturated sandstone')\n",
    "    im = ax.scatter(vs_km_s, vp_km_s, s=10, c=color_mapping[color], marker='.', cmap='inferno', alpha=0.8)\n",
    "    ax.set_xlabel('Vs (km/s)')\n",
    "    ax.set_ylabel('Vp (km/s)')\n",
    "    ax.set_title('Vp vs. Vs', fontsize=20, fontweight='bold')\n",
    "    ax.set_xlim([np.nanmin(vs_km_s), np.nanmax(vs_km_s)])\n",
    "    ax.set_ylim([np.nanmin(vp_km_s), np.nanmax(vp_km_s)])\n",
    "    ax.legend(loc='upper left')\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.ax.invert_yaxis()\n",
    "    cbar.set_label(f'{color}', fontdict={'fontweight':'bold'})\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Training Real vs. Training Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_pred(y, y_pred, feat_name, split):\n",
    "    \"\"\"Function that plots y vs y_pred\n",
    "    \n",
    "    Parameters:\n",
    "    y (ndarray like) : real values\n",
    "    y_pred (ndarray like) : predicted values\n",
    "    feat_name (str) : Feature name\n",
    "    split (str) : Whether this is 'Train', 'Test', or 'Full Log'\n",
    "    \n",
    "    returns matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    o2o = np.linspace(0, np.nanmax(y), len(y))\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    fig.suptitle('Real {} vs. Predict'.format(split))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.plot(y.reset_index(drop=True, inplace=False), 'k', lw=2, label='y {}'.format(split))\n",
    "    ax1.plot(y_pred, 'r', lw=1, label='y_{}_pred'.format(split))\n",
    "    ax1.set_xlabel('sample no.')\n",
    "    ax1.set_ylabel(feat_name)\n",
    "    ax1.set_title('{} Real & Predict'.format(split))\n",
    "    ax1.legend(loc='best')\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.scatter(y, y_pred, s=10, marker='.', alpha=0.8)\n",
    "    ax2.plot(o2o, o2o, 'k', lw=2, label='one-to-one')\n",
    "    ax2.set_xlabel('{} Real {}'.format(feat_name, split))\n",
    "    ax2.set_ylabel('{} Predict {}'.format(feat_name, split))\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_title('{} {} Predict vs. Real'.format(feat_name, split))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to visually QC original vs. imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_imputed(df_original, df_imputed):\n",
    "    \"\"\"Plots original and imputed log curves for visual QC\n",
    "    \n",
    "    Parameters:\n",
    "    df_original (pandas.DataFrame) : Input data frame containing the well log curves, one log per column\n",
    "    \n",
    "    df_imputed (pandas.DataFrame) : Input data frame containing filtered log curves, one log per column\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the column names as a list, assumes col names are the same for each input data frame\n",
    "    curve_names = df_original.columns.tolist()\n",
    "    \n",
    "    # build the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Original vs Imputed Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(df_imputed[curve], df_imputed.index, color='r', lw=0.5, label='imputed')\n",
    "            ax.semilogx(df_original[curve], df_original.index, color='k', lw=2, label='original')\n",
    "        else:\n",
    "            ax.plot(df_imputed[curve], df_imputed.index, color='r', lw=0.5, label='imputed')\n",
    "            ax.plot(df_original[curve], df_original.index, color='k', lw=2, label='original')\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "        ax.legend(loc='upper left')\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to find consecutive NaN values and the starting index number\n",
    "[Consecutive NaN larger than threshold](https://stackoverflow.com/questions46007776/consecutive-nan-larger-than-threshold-in-pandas-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_nans(index, col, threshold):\n",
    "    \"\"\"Function returns the starting index of consecutive NaN's in an input column\n",
    "    Uses Divakar's answer in above link\n",
    "    \"\"\"\n",
    "    \n",
    "    thresh = threshold\n",
    "    \n",
    "    a = index.values\n",
    "    b = col.values\n",
    "    \n",
    "    idx0 = np.flatnonzero(np.r_[True, np.diff(np.isnan(b))!=0, True])\n",
    "    count = np.diff(idx0)\n",
    "    idx = idx0[:-1]\n",
    "    valid_mask = (count>=thresh) & np.isnan(b[idx])\n",
    "    out_idx = idx[valid_mask]\n",
    "    out_num = a[out_idx]\n",
    "    out_count = count[valid_mask]\n",
    "    out = range(int(out_num),int(out_num)+int(out_count)-1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot custom [heatmap](https://github.com/amueller/mglearn/blob/master/mglearn/tools.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                               img.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            c = 'k'\n",
    "        else:\n",
    "            c = 'w'\n",
    "        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scorer functions for multi-output scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scorer = make_scorer(r2_score, multioutput='uniform_average')\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, multioutput='uniform_average', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"read-data\"></a>Read in Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1 = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"inspect-data\"></a>Inspect Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data are type float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples with value of -999.0 need to be replaced with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.replace(to_replace=-999.0, value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some suspicious values here that we will explore in the next step.\n",
    "\n",
    "1. CNC should range from 0.0 to 1.0\n",
    "1. GR should not have values less than 0.0\n",
    "1. PE should not have values less than 0.0\n",
    "1. ZDEN should not have values less than 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"eda\"></a>4. Exploratory Data Analysis (EDA) & Cleaning\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making any edits to values, plot the data for visual QC inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a [RHOMAA-UMAA crossplot](http://www.kgs.ku.edu/Publications/Bulletins/LA/11_crossplot.html) to see if we can differentiate lithologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_w1['PE'], df_w1['ZDEN'], df_w1['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely outliers are present!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit CNC to range between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['CNC'] < 0.0, ['CNC']] = np.nan\n",
    "df_w1.loc[df_w1['CNC'] > 1.0, ['CNC']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit GR to range between 0.0 and 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['GR'] < 0.0, ['GR']] = np.nan\n",
    "df_w1.loc[df_w1['GR'] > 300.0, ['GR']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit PE to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['PE'] < 0.0, ['PE']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit ZDEN to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['ZDEN'] < 0.0, ['ZDEN']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These value ranges are substantially more acceptable.  Re-plot the data and visually inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this appears to be better.  The PE final 10,000 values look suspect, though.\n",
    "\n",
    "**Let's attempt filtering out the spikes remaining in the data.**\n",
    "\n",
    "The filter first calculates an n-moving window median filtered version of the data and then eliminates values that fall outside of +/- n-standard deviations.\n",
    "\n",
    "It might make sense to first standardize the data using RobustScaler (uses interquartile range 25-75% of the data) and pass the standardized data to the filter.\n",
    "\n",
    "After filter, the data can be transformed back to its non-standardized domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "scaled_features = rs.fit_transform(df_w1.values)\n",
    "scaled_features_df = pd.DataFrame(data=scaled_features, index=df_w1.index, columns=df_w1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(scaled_features_df.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(scaled_features_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the scaled data to the filter and then backtransform the filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_scaled, df_outliers_scaled = filter_curves(scaled_features_df, 81, 2)\n",
    "\n",
    "# Backtransform the cleaned data & outliers to the unscaled domain\n",
    "clean_unscaled = rs.inverse_transform(df_clean_scaled.values)\n",
    "df_clean = pd.DataFrame(data=clean_unscaled, columns=df_w1.columns)\n",
    "outliers_unscaled = rs.inverse_transform(df_outliers_scaled.values)\n",
    "df_outliers = pd.DataFrame(data=outliers_unscaled, columns=df_w1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_fig = plot_well_curves(df_clean)\n",
    "cleaned_fig = plot_well_curves(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qc_fig = qc_curve_filt(df_w1, df_clean)\n",
    "qc_fig = qc_curve_filt(df_w1, df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "#qc_fig = qc_curve_filt(df_w1, df_clean, df_outliers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, it seems prudent to make some plots comparing our final cleaned curves to established rock property trends as a QC of the imputation and filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use plot_vp_vs to plot the imputed data against well known lithology trends, first the *pre-imputation* data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vp_vs(x='DTC', y='DTS', df=df_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few observations:**\n",
    "\n",
    "1. Limestone appears to be a significant lithology.\n",
    "\n",
    "1. Shale / Claystone also appear to be present throughout the well.\n",
    "\n",
    "1. Sands are present.  Some sands may be difficult to discriminate in the overlap zone between Shale, Sand, & Limestone lines.  Gas-charged sands fall below the water-saturated sandstone line.\n",
    "\n",
    "1. There may be a minor amount of dolomite present in the system.\n",
    "\n",
    "1. Vp & Vs trends are obviously dependent upon lithology.  Plotting all of the data together, there is obvious heteroscedasticity.  Linear models would not fare well directly predicting Vs from Vp.  However, by using other features which are sensitive to lithologic variations, we should be able to build a robust model to handle these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC lithologies with RHOMAA-UMAA crossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,_,_ = plot_rhomaa_umaa(df_clean['PE'].interpolate(limit_area='inside'), df_clean['ZDEN'].interpolate(limit_area='inside'), df_clean['CNC'].interpolate(limit_area='inside'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering operation has done a good job of removing outliers while keeping the overall signal intact\n",
    "\n",
    "PE definitely has some bad values, as evident both on the log plot and the RHOMAA-UMAA transform crossplot.\n",
    "\n",
    "**CAL seems a little suspect, though.  It seems unusual for CAL to increase at the bottom of the hole**\n",
    "\n",
    "In fact, CAL and PE basically go bad at the same sample number, so it is likely they had a shared tool failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to proceed.\n",
    "\n",
    "A few observations:\n",
    "\n",
    "* CAL could be useful (variations in the wellbore are related to geomechanical properties), but it can also be rather stationary.\n",
    "* If we want to maximize the amount of data available for estimating DTC & DTS, then interpolation is necessary.\n",
    "* The last 10,000 samples in the PE curve appear to be bogus.  Maybe these can be estimated using the shallower section?\n",
    "* HRM (Medium Resisitivity) should be dropped.  Without knowing the depth of investigation, it is likely to be contaminated with well-bore fluids.  HRD (Deep Resisitivity) is the better resistivity curve to use.  Maybe we could calculate Rt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several predictors exhibit a degree of multicollinearity\n",
    "\n",
    "* CNC & GR, which also exhibits heteroscedasticity\n",
    "* CNC & ZDEN\n",
    "* HRM & HRD\n",
    "\n",
    "Feature ranking will be an important step\n",
    "\n",
    "PCA may also be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Correlation Coefficient Heatmap of Well Log Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* CAL has reasonably high positive correlation to both DTC & DTS, as well as CNC.\n",
    "* CNC has reasonably high positive correlation to CAL, GR, PE, as well as to DTC & DTS\n",
    "* GR has moderate positive correlation to DTC & DTS\n",
    "* HRD & HRM do not exhibit strong correlation with any other parameters than themselves, *but this could be due to the fact they are log-scale*\n",
    "* PE has moderately positive correlation with CNC, DTC, & DTS\n",
    "* ZDEN has a high negative correlation with CNC, DTC, & DTS, and mild negative correlation with CAL, GR, & PE\n",
    "\n",
    "The recommendation going forward will be to drop CAL & HRM from the set of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdda",
   "language": "python",
   "name": "pdda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
