{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"top\"></a>SWPLA PDDA 2020 Synthetic Sonic Log Generation Contest\n",
    "\n",
    "**Author:** Ben Dowdell\n",
    "\n",
    "**Date:** March XX, 2020\n",
    "\n",
    "**Purpose:** To minimize RMSE in predicting Sonic logs (compressional & shear) from a suite of standard well logs\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [1. Initial Set-Up](#initial-setup)\n",
    "** [1a. Standard Imports](#standard-imports)\n",
    "** [1b. Sklearn Imports](#sklearn-imports)\n",
    "** [1c. Helper Function Definitions](#helper-funcs)\n",
    "* [2. Read Data](#read-data)\n",
    "* [3. Inspect Data](#inspect-data)\n",
    "* [4. EDA](#eda)\n",
    "* [5. Feature Engineering](#fe)\n",
    "** [5a. PE estimation](#fe-pe)\n",
    "** [5b. DTC approximation](#fe-dtc)\n",
    "** [5c. DTS approximation](#fe-dts)\n",
    "\n",
    "### Data Decription\n",
    "#### Files\n",
    "#### train.csv\n",
    "All the values equals to -999 are marked as missing values.\n",
    "- CAL - Caliper, unit in Inch,  \n",
    "- CNC - Neutron, unit in dec \n",
    "- GR - Gamma Ray, unit in API\n",
    "- HRD - Deep Resisitivity, unit in Ohm per meter,\n",
    "- HRM - Medium Resistivity, unit in Ohm per meter,\n",
    "- PE - Photo-electric Factor, unit in Barn,\n",
    "- ZDEN - Density, unit in Gram per cubit meter, \n",
    "- DTC - Compressional Travel-time, unit in nanosecond per foot,\n",
    "- DTS - Shear Travel-time, unit in nanosecond per foot,\n",
    "\n",
    "\n",
    "#### test.csv\n",
    "The test data has all features that you used in the train dataset, except the two sonic curves DTC and DTS.\n",
    "\n",
    "####  sample_submission.csv\n",
    "A valid sample submission.\n",
    "<p><font style=\"\">\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"initial-setup\"></a>1. Initial Set-Up\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"standard-imports\"></a> 1a. Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from pprint import pprint\n",
    "\n",
    "random_state = 42\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sklearn-imports\"></a>1b. Sklearn Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Imputer for missing values\n",
    "#from sklearn.experimental import enable_iterative_imputer\n",
    "#from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Preprocessing utilities\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Visualization\n",
    "from yellowbrick.model_selection import FeatureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"helper-funcs\"></a>1c. Helper Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot data histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distributions(df):\n",
    "    \"\"\"Plot histograms of each curve\n",
    "    \n",
    "    Paramters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,8))\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df_w1[col])\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function plot data CDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_cdf(df):\n",
    "    \"\"\"Plot cumulative distribution function of each curve\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame) : Input data frame containing log curves\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18,8))\n",
    "    for ax, col in zip(axes.flatten(), df.columns.tolist()):\n",
    "        ax.hist(df_w1[col], density=True, cumulative=-1)\n",
    "        if 'HR' in col:\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_title('{} Histogram'.format(col))\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot well curves in a normal log display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_well_curves(data):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : Input data frame containing the well log curves, one per column\n",
    "    \n",
    "    curve_names (list) : A list containing the column name of each well log curve in the input data frame\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # get the column names as a list\n",
    "    curve_names = data.columns.tolist()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data[curve], data.index, color='k')\n",
    "        else:\n",
    "            ax.plot(data[curve], data.index, color='k')\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS'] or 'DT' in curve:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Short Time Fourier Transform of the well curves (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stft_well_curves(data):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    Overlays the well log curve on top of a STFT representation of the signal\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : Input data frame containing the well log curves, one per column\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # get the column names as a list\n",
    "    curve_names = data.columns.tolist()\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        f, t, Zxx = signal.stft(data[curve], fs=1)\n",
    "        ax.pcolormesh(f.T, t.T, np.abs(Zxx).T, vmin=0, vmax=0.5)\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax1 = ax.twiny()\n",
    "            ax1.semilogx(data[curve], data.index, color='k')\n",
    "        else:\n",
    "            ax1 = ax.twiny()\n",
    "            ax1.plot(data[curve], data.index, color='k')\n",
    "        if curve == 'CNC':\n",
    "            ax1.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.invert_xaxis()\n",
    "            ax1.set_title(curve, fontdict={'color':'r'})\n",
    "            ax1.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax1.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to remove outliers from log curves using standard deviation from median value in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_curves(data, window, n_std):\n",
    "    \"\"\"\n",
    "    Takes a data frame containing well log curves and filters outliers based on n-standard deviations\n",
    "    from a median filtered version of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame) : The input data frame containing data to be filtered.  Each column is a well log curve.\n",
    "    \n",
    "    window (int) : The size of the window to use in creating a median filtered curve (recommend 33)\n",
    "    \n",
    "    n_std (int) : The number of +/- standard deviations to use in considering outliers (recommend 2)\n",
    "    \n",
    "    Returns:\n",
    "    df_clean (pandas.DataFrame) : The filtered well curves\n",
    "    df_outliers (pandas.DataFrame) : The outlier data points removed by the filtering operation\n",
    "    \"\"\"\n",
    "    # create a copy of the original data\n",
    "    df_copy = data.copy()\n",
    "    \n",
    "    # create a data frame containing median-filtered version of the data\n",
    "    df_medfilter = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_medfilter[col] = df_copy[col].rolling(window, center=True).median()\n",
    "        \n",
    "    # create a data frame containing standard deviation of the data\n",
    "    df_stddev = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        df_stddev[col] = df_medfilter[col].rolling(window, center=True).std()\n",
    "        \n",
    "    # create a data frame containing the cleaned version of the data using standard deviation from the median filtered data\n",
    "    # and create a data frame containing the removed outliers\n",
    "    df_clean = pd.DataFrame()\n",
    "    df_outliers = pd.DataFrame()\n",
    "    for col in df_copy.columns.tolist():\n",
    "        upper = df_medfilter[col] + df_stddev[col]*n_std\n",
    "        lower = df_medfilter[col] - df_stddev[col]*n_std\n",
    "        df_clean[col] = df_copy[col].where((df_copy[col] <= upper) & (df_copy[col] >= lower))\n",
    "        #df_clean[col] = df_clean[col].interpolate(limit_area='inside')\n",
    "        df_outliers[col] = df_copy[col].where(df_clean[col] != df_copy[col])\n",
    "    \n",
    "    return df_clean, df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare raw versus filtered logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_curve_filt(data_raw, data_cleaned, *args):\n",
    "    \"\"\"\n",
    "    Plots well log curves from an input data frame containing the data\n",
    "    \n",
    "    Parameters:\n",
    "    data_raw (pandas.DataFrame) : Input data frame containing the well log curves, one log per column\n",
    "    \n",
    "    data_cleaned (pandas.DataFrame) : Input data frame containing filtered log curves, one log per column\n",
    "    \n",
    "    *args (pandas.DataFrame) : Optional data frame containing outliers\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the column names as a list, assumes col names are the same for each input data frame\n",
    "    curve_names = data_raw.columns.tolist()\n",
    "    \n",
    "    # build the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(curve_names), sharey=True, figsize=(20,10))\n",
    "    fig.suptitle('Training Well Data', fontsize=20)\n",
    "    for ax, curve in zip(axes, curve_names):\n",
    "        if curve in ['HRD', 'HRM']:\n",
    "            ax.semilogx(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.semilogx(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.semilogx(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        else:\n",
    "            ax.plot(data_raw[curve], data_raw.index, color='k', lw=3)\n",
    "            ax.plot(data_cleaned[curve], data_cleaned.index, color='r', lw=1)\n",
    "            for arg in args:\n",
    "                ax.plot(arg[curve], arg.index, lw=0, marker='.', mfc='y', mec='k', alpha=0.2)\n",
    "        if curve == 'CNC':\n",
    "            ax.set_xlim(0.0, 1.0)\n",
    "        if curve in ['DTC', 'DTS']:\n",
    "            ax.set_title(curve, fontdict={'color':'r'})\n",
    "            ax.invert_xaxis()\n",
    "        else:\n",
    "            ax.set_title(curve)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.invert_yaxis()\n",
    "    fig.text(0.04, 0.5, 'Sample', va='center', rotation='vertical', fontdict={'fontsize':20})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to compare individual predictors to DTC & DTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_curves(predictor, dtc, dts):\n",
    "    \"\"\"\n",
    "    Takes a predictor curve and plots it against both response variables, DTC & DTS\n",
    "    \n",
    "    Parameters:\n",
    "    predictor (pandas.core.series.Series) : Independent Variable to compare, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dtc (pandas.core.series.Series) : DTC curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    dts (pandas.core.series.Series) : DTS curve, assumes either a column from a pandas DataFrame or ndarray-like\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,10), linewidth=5, edgecolor='k')\n",
    "    name = predictor.name\n",
    "    fig.suptitle('{} curve comparison to DTC & DTS'.format(name), fontsize=20)\n",
    "    \n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    if name == 'HRD':\n",
    "        ax1.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax1.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax1c = ax1.twinx()\n",
    "    ax1c.plot(dtc.index, dtc.values, 'r', lw=2)\n",
    "    ax1c.invert_yaxis()\n",
    "    ax1.set_ylabel(name)\n",
    "    ax1c.set_ylabel('DTC', fontdict={'color':'r'})\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2 = sns.scatterplot(x=predictor, y=dtc)\n",
    "    if name == 'HRD':\n",
    "        ax2.set_xscale('log')\n",
    "    ax2.set_xlabel(name)\n",
    "    ax2.set_ylabel('DTC')\n",
    "    \n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    if name == 'HRD':\n",
    "        ax3.semilogy(predictor.index, predictor.values, 'k', lw=3)\n",
    "    else:\n",
    "        ax3.plot(predictor.index, predictor.values, 'k', lw=3)\n",
    "    ax3c = ax3.twinx()\n",
    "    ax3c.plot(dts.index, dts.values, 'r', lw=2)\n",
    "    ax3c.invert_yaxis()\n",
    "    ax3.set_ylabel(name)\n",
    "    ax3c.set_ylabel('DTS', fontdict={'color':'r'})\n",
    "    \n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    ax4 = sns.scatterplot(x=predictor, y=dts)\n",
    "    if name == 'HRD':\n",
    "        ax4.set_xscale('log')\n",
    "    ax4.set_xlabel(name)\n",
    "    ax4.set_ylabel('DTS')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to calculate Root Mean Squared Error (RMSE), the primary evaluation metric for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(real, predicted):\n",
    "    \"\"\"Calculates the Root Mean Square Error\n",
    "     \n",
    "     Parameters:\n",
    "     real (ndarray) : actual values as a numpy array\n",
    "     predicted (ndarray) : predicted values as a numpy array\n",
    "     \n",
    "     Returns: RMSE Accuracy as a float\n",
    "     \n",
    "    \"\"\"\n",
    "    mse = np.square(np.subtract(real, predicted)).mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot logs in a typical petrophysical layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def petro_plot(gr, resd, neut, den, pe, dtc, dts):\n",
    "    \"\"\"\n",
    "    Produces a typical petrophysical plot\n",
    "    \n",
    "    Parameters:\n",
    "    gr (pandas.Series) : Gamma Ray curve as a pandas series\n",
    "    resd (pandas.Series) : Deep Resistivity curve as a pandas series\n",
    "    neut (pandas.Series) : Neutron Porosity curve as a pandas series\n",
    "    den (pandas.Series) : Density curve as a pandas series\n",
    "    pe (pandas.Series) : PE curve as a pandas series\n",
    "    dtc (pandas.Series) : Compressional sonic curve as a pandas series\n",
    "    dts (pandas.Series) : Shear sonic curve as a pandas series\n",
    "    \n",
    "    Returns : matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "\n",
    "    # GR\n",
    "    ax1 = fig.add_subplot(1,5,1)\n",
    "    ax1.plot(gr, gr.index, 'k', lw=1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('GR')\n",
    "    ax1.set_ylabel('Sample no.')\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.xaxis.set_label_position('top')\n",
    "\n",
    "    # RD\n",
    "    ax2 = fig.add_subplot(1,5,2)\n",
    "    ax2.semilogx(resd, resd.index, 'b', lw=1)\n",
    "    ax2.set_xlabel('HRD')\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.xaxis.tick_top()\n",
    "    ax2.xaxis.set_label_position('top')\n",
    "\n",
    "    # N-D\n",
    "    ax3 = fig.add_subplot(1,5,3)\n",
    "    ax3.plot(den, den.index, 'r', lw=1)\n",
    "    ax3.xaxis.set_label_position('top')\n",
    "    ax3.set_xlabel('ZDEN', labelpad=10, fontdict={'color':'r'})\n",
    "    ax3.set_xlim(1.7, 2.7)\n",
    "    ax3.invert_yaxis()\n",
    "    ax3b = ax3.twiny()\n",
    "    ax3b.plot(neut, neut.index, 'g', lw=1)\n",
    "    ax3b.invert_xaxis()\n",
    "    ax3b.set_xlabel('CNC', fontdict={'color':'g'})\n",
    "    ax3b.set_xlim(0.6, 0.0)\n",
    "    ax3b.xaxis.tick_top()\n",
    "    ax3.xaxis.tick_top()\n",
    "    ax3.set_xticks([1.7, 1.9, 2.1, 2.3, 2.5, 2.7])\n",
    "    ax3.tick_params(axis='x', pad=35, top=True)\n",
    "    ax3b.xaxis.set_label_position('top')\n",
    "    ax3b.set_xticks([0.6, 0.48, 0.36, 0.24, 0.12, 0.0])\n",
    "\n",
    "    # PE\n",
    "    ax4 = fig.add_subplot(1,5,4)\n",
    "    ax4.plot(pe, pe.index, 'k', lw=1)\n",
    "    ax4.set_xlabel('PE')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "\n",
    "    # Sonic\n",
    "    ax5 = fig.add_subplot(1,5,5)\n",
    "    ax5.plot(dtc, dtc.index, 'b', lw=1)\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    ax5.set_xlabel('DTC', labelpad=10, fontdict={'color':'b'})\n",
    "    ax5.set_xlim(50, 170)\n",
    "    ax5.invert_xaxis()\n",
    "    ax5.invert_yaxis()\n",
    "    ax5b = ax5.twiny()\n",
    "    ax5b.plot(dts, dts.index, 'r', lw=1)\n",
    "    ax5b.invert_xaxis()\n",
    "    ax5b.set_xlabel('DTS', fontdict={'color':'r'})\n",
    "    ax5b.set_xlim(340, 100)\n",
    "    ax5b.xaxis.tick_top()\n",
    "    ax5.xaxis.tick_top()\n",
    "    #ax5.set_xticks([170, 150, 130, 110, 90, 70, 50])\n",
    "    ax5.set_xticks([230, 200, 170, 140, 110, 80, 50])\n",
    "    ax5.tick_params(axis='x', pad=35, top=True)\n",
    "    ax5b.xaxis.set_label_position('top')\n",
    "    #ax5b.set_xticks([340, 300, 260, 220, 180, 140, 100])\n",
    "    ax5b.set_xticks([460, 400, 340, 280, 220, 160, 100])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot RHOMAA-UMAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rhomaa_umaa(pe, dens, phi):\n",
    "    \"\"\"Plot log data using RHOMAA-UMAA crossplot for mineralogy\n",
    "    \n",
    "    Parameters:\n",
    "    pe (ndarray-like) : PE curve\n",
    "    dens (ndarray-like) : Density curve\n",
    "    phi (ndarray-like) : Neutron porosity curve\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    # First, create the endmember points as a dataframe\n",
    "    index = ['Quartz', 'Calcite', 'Dolomite', 'Anhydrite', 'K-Feldspar']\n",
    "    data = {\n",
    "        'rhoma' : [2.65, 2.71, 2.87, 2.95, 2.54],\n",
    "        'uma' : [4.82, 13.79, 8.98, 14.99, 7.29],\n",
    "    }\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    \n",
    "    # Second, calculate umaa & rhomaa for the input logs\n",
    "    u = pe * dens\n",
    "    phid = (2.71 - dens)/(2.71-1.)\n",
    "    phit = (phid + phi)/2\n",
    "    #umaa = u / (1 - phit)\n",
    "    #umaa = (u - phit*0.5)/(1-phit)/2\n",
    "    rhomaa = (dens - phit*1.)/(1-phit) # assume brackish pore fluid rhof = 1.1 g/cc\n",
    "    umaa = (pe * rhomaa) / 2\n",
    "    \n",
    "    # Finally, create and return figure\n",
    "    fig = plt.figure(figsize=(28,10))\n",
    "    \n",
    "    gs = fig.add_gridspec(1, 6)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0,:-3])\n",
    "    im = ax1.scatter(umaa, rhomaa, s=10, c=umaa.index, marker='.', cmap='inferno', alpha=0.8)\n",
    "    ax1.scatter(df['uma'], df['rhoma'], c='r', marker='D')\n",
    "    for i, txt in enumerate(df.index.tolist()):\n",
    "        ax1.annotate(txt, (df['uma'].iloc[i]+0.03, df['rhoma'].iloc[i]-0.02), weight='bold')\n",
    "    ax1.plot(df['uma'].loc[['Quartz','Calcite']], df['rhoma'].loc[['Quartz', 'Calcite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Quartz', 'Dolomite']], df['rhoma'].loc[['Quartz', 'Dolomite']], 'k')\n",
    "    ax1.plot(df['uma'].loc[['Dolomite','Calcite']], df['rhoma'].loc[['Dolomite','Calcite']], 'k')\n",
    "    ax1.set_xlabel('UMAA (barns/cc)')\n",
    "    #ax1.set_xlim(2,16)\n",
    "    ax1.set_ylabel('RHOMAA (g/cc)')\n",
    "    #ax1.set_ylim(2.3, 3.1)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_title('RHOMAA-UMAA Crossplot', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "    cbar = fig.colorbar(im, ax=ax1)\n",
    "    cbar.ax.invert_yaxis()\n",
    "    cbar.set_label('sample no', fontdict={'fontweight':'bold'})\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0,3])\n",
    "    ax4.plot(phit, phit.index, 'k', lw=2, label='phit')\n",
    "    ax4.plot(phi, phi.index, 'r', lw=0.5, label='phin')\n",
    "    ax4.plot(phid, phid.index, 'g', lw=0.5, label='phid')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.set_ylabel('sample no')\n",
    "    ax4.invert_xaxis()\n",
    "    ax4.set_xlabel('Phi')\n",
    "    ax4.xaxis.tick_top()\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "    ax4.legend(loc='best')\n",
    "    \n",
    "    ax5 = fig.add_subplot(gs[0,4], yticklabels=())\n",
    "    ax5.plot(umaa, umaa.index)\n",
    "    ax5.invert_yaxis()\n",
    "    ax5.set_xlabel('UMAA')\n",
    "    ax5.xaxis.tick_top()\n",
    "    ax5.xaxis.set_label_position('top')\n",
    "    \n",
    "    ax6 = fig.add_subplot(gs[0,5], yticklabels=())\n",
    "    ax6.plot(rhomaa, rhomaa.index)\n",
    "    ax6.invert_yaxis()\n",
    "    ax6.set_xlabel('RHOMAA')\n",
    "    ax6.xaxis.tick_top()\n",
    "    ax6.xaxis.set_label_position('top')\n",
    "    \n",
    "    return fig\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot Training Real vs. Training Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_pred(y, y_pred, feat_name, split):\n",
    "    \"\"\"Function that plots y vs y_pred\n",
    "    \n",
    "    Parameters:\n",
    "    y (ndarray like) : real values\n",
    "    y_pred (ndarray like) : predicted values\n",
    "    feat_name (str) : Feature name\n",
    "    split (str) : Whether this is 'Train', 'Test', or 'Full Log'\n",
    "    \n",
    "    returns matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    fig.suptitle('Real {} vs. Predict'.format(split))\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.plot(y.reset_index(drop=True, inplace=False), 'k', lw=2, label='y {}'.format(split))\n",
    "    ax1.plot(y_pred, 'r', lw=1, label='y_{}_pred'.format(split))\n",
    "    ax1.set_xlabel('sample no.')\n",
    "    ax1.set_ylabel(feat_name)\n",
    "    ax1.set_title('{} Real & Predict'.format(split))\n",
    "    ax1.legend(loc='best')\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.scatter(y, y_pred)\n",
    "    ax2.set_xlabel('{} Real {}'.format(feat_name, split))\n",
    "    ax2.set_ylabel('{} Predict {}'.format(feat_name, split))\n",
    "    ax2.set_title('{} {} Predict vs. Real'.format(feat_name, split))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"read-data\"></a>Read in Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1 = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"inspect-data\"></a>Inspect Data\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data are type float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples with value of -999.0 need to be replaced with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.replace(to_replace=-999.0, value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some suspicious values here that we will explore in the next step.\n",
    "\n",
    "1. CNC should range from 0.0 to 1.0\n",
    "1. GR should not have values less than 0.0\n",
    "1. PE should not have values less than 0.0\n",
    "1. ZDEN should not have values less than 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"eda\"></a>4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Back to [top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making any edits to values, plot the data for visual QC inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a [RHOMAA-UMAA crossplot](http://www.kgs.ku.edu/Publications/Bulletins/LA/11_crossplot.html) to see if we can differentiate lithologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_w1['PE'], df_w1['ZDEN'], df_w1['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely outliers are present!\n",
    "\n",
    "For fun, lets see what a Short-Time Fourier Transform (STFT) of the well logs look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_plot = plot_stft_well_curves(df_w1.interpolate(limit_area='inside'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit CNC to range between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['CNC'] < 0.0, ['CNC']] = np.nan\n",
    "df_w1.loc[df_w1['CNC'] > 1.0, ['CNC']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit GR to range between 0.0 and 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['GR'] < 0.0, ['GR']] = np.nan\n",
    "df_w1.loc[df_w1['GR'] > 300.0, ['GR']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit PE to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['PE'] < 0.0, ['PE']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit ZDEN to have values no less than 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.loc[df_w1['ZDEN'] < 0.0, ['ZDEN']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These value ranges are substantially more acceptable.  Re-plot the data and visually inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_plot_fig = plot_well_curves(df_w1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this appears to be better.  The PE final 10,000 values look suspect, though.\n",
    "\n",
    "Let's attempt filtering out the spikes remaining in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean, df_outliers = filter_curves(df_w1, 27, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w1.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_fig = plot_well_curves(df_clean)\n",
    "#cleaned_fig = plot_well_curves(df_clean.fillna(method='bfill').fillna(method='ffill'))\n",
    "cleaned_fig = plot_well_curves(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "#cleaned_fig = plot_well_curves(df_clean.dropna(axis=0, inplace=False).interpolate(limit_area='inside'))\n",
    "#cleaned_fig = plot_well_curves(df_clean.dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qc_fig = qc_curve_filt(df_w1, df_clean)\n",
    "qc_fig = qc_curve_filt(df_w1, df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "#qc_fig = qc_curve_filt(df_w1, df_clean, df_outliers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_clean['PE'].interpolate(limit_area='inside'), df_clean['ZDEN'].interpolate(limit_area='inside'), df_clean['CNC'].interpolate(limit_area='inside'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering operation has done a good job of removing outliers while keeping the overall signal intact\n",
    "\n",
    "PE definitely has some bad values, as evident both on the log plot and the RHOMAA-UMAA transform crossplot.\n",
    "\n",
    "**CAL seems a little suspect, though.  It seems unusual for CAL to increase at the bottom of the hole**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False).describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to proceed.\n",
    "\n",
    "A few observations:\n",
    "\n",
    "* CAL could be useful (variations in the wellbore are related to geomechanical properties), but it can also be rather stationary.\n",
    "* If we want to maximize the amount of data available for estimating DTC & DTS, then interpolation is necessary.\n",
    "* The last 10,000 samples in the PE curve appear to be bogus.  Maybe these can be estimated using the shallower section?\n",
    "* HRM (Medium Resisitivity) should be dropped.  Without knowing the depth of investigation, it is likely to be contaminated with well-bore fluids.  HRD (Deep Resisitivity) is the better resistivity curve to use.  Maybe we could calculate Rt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several predictors exhibit a degree of multicollinearity\n",
    "\n",
    "* CNC & GR, which also exhibits heteroscedasticity\n",
    "* CNC & ZDEN\n",
    "* HRM & HRD\n",
    "\n",
    "Feature ranking will be an important step\n",
    "\n",
    "PCA may also be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_clean.interpolate(limit_area='inside').dropna(axis=0, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Correlation Coefficient Heatmap of Well Log Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* CAL has reasonably high positive correlation to both DTC & DTS, as well as CNC.\n",
    "* CNC has reasonably high positive correlation to CAL, GR, PE, as well as to DTC & DTS\n",
    "* GR has moderate positive correlation to DTC & DTS\n",
    "* HRD & HRM do not exhibit strong correlation with any other parameters than themselves, *but this could be due to the fact they are log-scale*\n",
    "* PE has moderately positive correlation with CNC, DTC, & DTS\n",
    "* ZDEN has a high negative correlation with CNC, DTC, & DTS, and mild negative correlation with CAL, GR, & PE\n",
    "\n",
    "The recommendation going forward will be to drop HRM from the set of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of df_clean to take log transform of HRD & HRM to examine how they correlate to the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_v2 = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_v2['HRD'] = df_clean_v2['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_v2['HRM'] = df_clean_v2['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_clean_v2.interpolate(limit_area='inside').dropna(axis=0, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_clean_v2.interpolate(limit_area='inside').dropna(axis=0, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Corr Coef Heatmap - log(HRx)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data dropping HRM as it provides duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_clean.copy().drop(labels='HRM', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.interpolate(limit_area='inside').dropna(axis=0, inplace=False).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_subset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_subset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is coincidentally a great example of lognormal-ness in a natural system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp = df_subset.copy().interpolate(limit_area='inside').dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_distributions(df_subset_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_cdf(df_subset_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df_subset_interp.columns.tolist()\n",
    "predictors = [x for x in all_features if x not in ['DTC', 'DTS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in predictors:\n",
    "    ax = compare_curves(df_subset_interp[col], df_subset_interp['DTC'], df_subset_interp['DTS'])\n",
    "#plt.savefig('feature_compare_to_sonic_curves.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gameplan:**\n",
    "\n",
    "1. Estimate missing PE for samples beyond 20000 and splice in\n",
    "1. Estimate missing DTC & DTS using either Faust or Gardner and splice in\n",
    "1. Build a data-driven model for estimating DTC & DTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"fe\"></a> 5. Feature Engineering\n",
    "\n",
    "Back to [top](#top)\n",
    "\n",
    "Before proceeding to estimating DTC & DTS, if we want to use as much of the data as possible, we need to attempt to estimate the bad values for PE as well as missing DTC & DTS.\n",
    "\n",
    "1. [5a. PE Curve Estimation](#fe-pe)\n",
    "1. [5b. DTC Approximation](#fe-dtc)\n",
    "1. [5c. DTS Approximation](#fe-dts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-pe\"></a> 5a. Feature Engineering - PE Curve Estimation\n",
    "\n",
    "Back to [top](#top)\n",
    "\n",
    "The goal is to estimate PE for the final 10,000 samples that appear to have a bad measurement.\n",
    "\n",
    "If we can robustly estimate the PE curve that appears to be usable, then we can splice in the estimated section for the bad data samples, keeping the good data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously observed that CNC & ZDEN were most correlated to PE, so take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_subset_interp[['CNC','PE','ZDEN']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth crossplotting PE and HRD using a semilog scale to see if there is any relation not immediately noticeable in linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sns.scatterplot(x='HRD', y='PE', data=df_subset_interp)\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify at which sample the values become constantly bad.  Maybe one way to do this is to look at the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp[df_subset_interp['PE'] < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick df filter operation suggests that the bad PE values start at index 19338."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.plot(df_subset_interp['PE'].index, np.gradient(df_subset_interp['PE']))\n",
    "ax1.scatter(19338, df_subset_interp['PE'].iloc[19338], c='k', s=100)\n",
    "ax1.set_xlabel('sample no.')\n",
    "ax1.set_ylabel('Gradient of PE')\n",
    "ax1.set_title('Gradient of PE vs. Sample No.')\n",
    "\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.plot(range(19200, 20000), np.gradient(df_subset_interp['PE'])[19200:20000])\n",
    "ax2.scatter(19338, df_subset_interp['PE'].iloc[19338], c='k', s=100)\n",
    "ax2.set_xlabel('sample no.')\n",
    "ax2.set_ylabel('Gradient of PE')\n",
    "ax2.set_title('Zoomed plot about PE Gradient Samples')\n",
    "\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "ax3.plot(np.gradient(np.gradient(df_subset_interp['PE'])))\n",
    "ax3.set_xlabel('sample no.')\n",
    "ax3.set_ylabel('Second derivative of PE')\n",
    "ax3.set_title('Second derivative of PE')\n",
    "\n",
    "ax4 = fig.add_subplot(2,2,4)\n",
    "ax4.plot(range(19200, 20000), np.gradient(np.gradient(df_subset_interp['PE']))[19200:20000], 'k', label='Second derivative')\n",
    "ax4.plot(range(19200, 20000), np.gradient(df_subset_interp['PE'])[19200:20000], 'r', label='First derivative')\n",
    "ax4.scatter(19338, df_subset_interp['PE'].iloc[19338], s=100)\n",
    "ax4.set_xlabel('Sample no.')\n",
    "ax4.set_ylabel('Second Derivative of PE')\n",
    "ax4.set_title('Second Derivative of PE')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection of the first and second derivative confirm that index 19338 is the start of the offending data.\n",
    "\n",
    "Index Number 19338 is the point at which PE values appear to go consistently bad.  We can use all data up to this point for attempting to estimate PE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe = df_subset_interp[['CNC','HRD','ZDEN','PE']].iloc[0:19338].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the log of 'HRD' to put it in the same scale as CNC, ZDEN, & PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe['HRD'] = df_pe['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.rename(columns={'HRD':'log_HRD'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.log(df_pe['HRD']).plot()\n",
    "df_pe['log_HRD'].plot()\n",
    "df_pe['PE'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train-Test-Split\n",
    "1. Randomized Parameter Search + Cross Validation (Hyperparameter Tuning)\n",
    "1. Grid-Search + K-Fold Cross Validation (Hyperparameter Fine Tuning)\n",
    "1. Model Evaluation\n",
    "1. QC real vs. predicted PE\n",
    "1. Splice predicted data in for bad data, keeping good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_pe[['CNC', 'log_HRD', 'ZDEN']], df_pe['PE'], test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a default Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a random grid for RandomizedSearchCV to select initial hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(start=10, stop=100, num=10)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=5, verbose = 2, random_state=random_state, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Root Mean Squared Error for RandomForestRegressor with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_random.best_estimator_.predict(X_train)\n",
    "print('Training RMSE: {}'.format(mean_squared_error(y_train, y_train_pred, squared=False)))\n",
    "print('R2 Score: {}'.format(r2_score(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Root Mean Squared Error for RandomForestRegressor with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_random.best_estimator_.predict(X_test)\n",
    "print('Testing RMSE: {}'.format(mean_squared_error(y_test, y_test_pred, squared=False)))\n",
    "print('R2 Score: {}'.format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to a non-tuned default RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_default = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default RFR Training Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_default_pred = rf_default.predict(X_train)\n",
    "print('Default RF Training RMSE: {}'.format(mean_squared_error(y_train, y_train_default_pred, squared=False)))\n",
    "print('Default RF R2 Score :{}'.format(r2_score(y_train, y_train_default_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default RFR Testing Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_default_pred = rf_default.predict(X_test)\n",
    "print('Default RF Testing RMSE: {}'.format(mean_squared_error(y_test, y_test_default_pred, squared=False)))\n",
    "print('Default RF R2 Score: {}'.format(r2_score(y_test, y_test_default_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial hyperparameter tuning using RandomizedSearchCV has improved the scores, but let's now use GridSearchCV to attempt to tune for a little more performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40, 45, 50, 55, 60],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=700, stop=900, num=21)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV Train RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_gs_pred = grid_search.best_estimator_.predict(X_train)\n",
    "print('RF GridSearch Best Estimator RMSE: {}'.format(mean_squared_error(y_train, y_train_gs_pred, squared=False)))\n",
    "print('RF GridSearch Best Estimator R2: {}'.format(r2_score(y_train, y_train_gs_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_gs_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print('RF GridSearch Best Estimator Test RMSE: {}'.format(mean_squared_error(y_test, y_test_gs_pred, squared=False)))\n",
    "print('RF GridSearch Best Estimator Test R2: {}'.format(r2_score(y_test, y_test_gs_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator from gridsearch performed slightly worse on the training data but slightly better on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a new RandomForestRegressor using the parameters of the GridSearchCV best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Yellowbrick to evaluate model performance further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(rf_best)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training vs. Predict & Test vs. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(y_train, y_train_gs_pred, feat_name='PE', split='Train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(y_test, y_test_gs_pred, feat_name='PE', split='Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the entire curve using the model to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_whole_gs_pred = rf_best.predict(df_pe[['CNC', 'log_HRD', 'ZDEN']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_pe['PE'], y_whole_gs_pred, feat_name='PE', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby for the clean section of the log!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the entire predicted log look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe_pred = pd.DataFrame()\n",
    "df_pe_pred['CNC'] = df_subset_interp['CNC']\n",
    "df_pe_pred['log_HRD'] = np.log(df_subset_interp['HRD'])\n",
    "df_pe_pred['ZDEN'] = df_subset_interp['ZDEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe_pred['PE_est'] = rf_best.predict(df_pe_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['PE'], df_pe_pred['PE_est'], feat_name='PE', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have what appears to be a robust estimate of the missing PE data, we can splice it in to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['PE_est'] = df_subset_interp['PE']\n",
    "df_subset_interp['PE_est'].iloc[19338:] = df_pe_pred['PE_est'].iloc[19338:]\n",
    "df_subset_interp['log_HRD'] = df_subset_interp['HRD'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['PE'], df_subset_interp['PE_est'], feat_name='PE', split='Full Log with Splicing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check RHOMAA-UMAA crossplot with predicted PE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rhomaa_umaa(df_subset_interp['PE_est'], df_subset_interp['ZDEN'], df_subset_interp['CNC'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_subset_interp.drop(labels='PE', axis=1, inplace=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_subset_interp.drop(labels=\"PE\", axis=1, inplace=False).corr(), cmap=\"RdBu_r\", annot=True, fmt=\".2f\")\n",
    "ax.set_title('Corr Coef Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-dtc\"></a> 5b. Feature Engineering - DTC Approximation\n",
    "\n",
    "back to [Feature Engineering](#fe)\n",
    "\n",
    "back to [top](#top)\n",
    "\n",
    "Next, we will attempt to approximate the missing DTC values where previously we have used a linear interpolation method.  There are several ways this could be accomplished.\n",
    "\n",
    "1. Use [Gardner's Equation](#gardner) (1974) : $\\rho = 0.23 * (V_{p}^{0.25})$, which can be re-written as $V_{p} = \\sqrt[0.25]{\\frac{\\rho}{0.23}}$\n",
    "1. Use Faust's Relationship (1951): $V_{p} = \\alpha*(R*Z)^{\\frac{1}{6}}$ where,\n",
    "\n",
    "    $\\alpha \\equiv$ a depth-related constant ranging from 1000-3400,\n",
    "    \n",
    "    $R \\equiv$ Resistivity, and\n",
    "    \n",
    "    $Z \\equiv$ Depth in feet\n",
    "    \n",
    "    *Unfortunately, Faust cannot be used here because we don't know actual depth, only sample number*\n",
    "    \n",
    "1. Use a [model approach](#dtc-model) using another curve(s) which tracks DTC closely.  For example, CNC correlates very highly with DTC.\n",
    "\n",
    "Recall that $V_{p} = \\frac{1E6}{DTC}$, and $DTC = \\frac{1E6}{V_{p}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"gardner\"></a> 5b.1 Gardner's Equation for estimating DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vp_gard = df_subset_interp['ZDEN'].apply(lambda x: np.power((x/0.23), 1/0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vp_raw = 1e6/df_subset_interp['DTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(Vp_raw, 'k', lw=2, label='Vp_raw')\n",
    "ax.plot(Vp_gard, 'r', lw=1, label='Vp_gard')\n",
    "ax.set_xlabel('Sample no.')\n",
    "ax.set_ylabel('Vp')\n",
    "ax.set_title('Vp vs. Vp from Density using Gardner\\'s Equation')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Root Mean Square Error of Gardner Equation prediction using ZDEN: {}'.format(evaluate_rmse(Vp_raw, Vp_gard)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"dtc-model\"></a> 5b.2 Model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,6))\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax1b = ax1.twinx()\n",
    "ax1b.plot(df_subset_interp['CNC'], 'r', lw=1, label='CNC')\n",
    "ax1.set_xlabel('Sample no.')\n",
    "ax1.set_ylabel('DTC')\n",
    "ax1b.set_ylabel('CNC', color='red')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.scatter(df_subset_interp['CNC'], df_subset_interp['DTC'])\n",
    "ax2.set_xlabel('CNC')\n",
    "ax2.set_ylabel('DTC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_subset_interp['CNC']).reshape(-1,1), np.array(df_subset_interp['DTC']).reshape(-1,1), test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR coeff: {}'.format(lr.coef_))\n",
    "print('LR intercept: {}'.format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR training score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('LR test score :{}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_cnc = lr.predict(np.array(df_subset_interp['CNC']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_cnc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['DTC'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTC'], DTC_cnc, feat_name='DTC', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTC from CNC: {}'.format(evaluate_rmse(df_subset_interp['DTC'], DTC_cnc[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This result is significantly more accurate compared to using the Gardner equation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,10))\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(df_subset_interp['DTC'], 'k', lw=3, label='DTC_raw')\n",
    "ax1.plot((1e6/Vp_gard), 'b', lw=1, label='DTC_gard')\n",
    "ax1.plot((DTC_cnc+rng.normal(0,1,DTC_cnc.shape)), '--g', lw=1, label='DTC_cnc+random noise')\n",
    "ax1.plot(DTC_cnc, 'r', lw=1, label='DTC_cnc')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_xlabel('sample no.')\n",
    "ax1.set_ylabel('DTC')\n",
    "ax1.set_title('Comparing raw DTC vs. two prediction methods')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression for DTC on CNC is a better estimator and should be used for the missing section.  Adding in some random noise is probably a good idea so it is not directly related to CNC**\n",
    "\n",
    "*Alternatively, each could be used as as separate final models... One could be interpolated values, One could be DTC_gard, and One could be DTC_cnc*\n",
    "\n",
    "**Now we need to splice in the DTC_cnc estimate for the zone that is interpolated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax.plot(np.round(np.gradient(df_subset_interp['DTC']), 1), 'r', lw=1, label='gradient')\n",
    "ax.plot(df_subset_interp['DTC'].diff(), 'b', lw=0.25, label='diff')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('DTC')\n",
    "ax.set_title('Gradient of DTC')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp[np.abs(np.round(np.gradient(df_subset_interp['DTC']), 1)) == 0 ].loc[8441:12551,'DTC'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTC'], 'k', lw=2, label='DTC')\n",
    "ax.plot(df_subset_interp['DTC'].iloc[8441:12551], 'r', lw=2, label='DTC interp')\n",
    "ax.plot(np.round(np.gradient(df_subset_interp['DTC']),1), 'r', lw=1, label='gradient')\n",
    "ax.scatter(df_subset_interp[np.abs(np.round(np.gradient(df_subset_interp['DTC']), 1)) == 0].loc[8441:12551,'DTC'].index, np.zeros(4108), s=20, c='b', marker='.', label='interpolated data')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTC')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is probably a more elegant way to find the index range of the interpolated data, but for now this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_interp['DTC_spliced'] = df_subset_interp['DTC']\n",
    "DTC_cnc_awgn = DTC_cnc + rng.normal(0,1,DTC_cnc.shape)\n",
    "df_subset_interp['DTC_spliced'].iloc[8441:12551] = DTC_cnc_awgn[8441:12551,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTC'], df_subset_interp['DTC_spliced'], feat_name='DTC', split='Full Log with Splicing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_well_curves(df_subset_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTC_spliced looks like a plausible estimation for the missing data section.  Notice the slow velocity layer just above sample number 10,000.  The same slow down is seen on the DTS log.  Encouraging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petro_plot(df_subset_interp['GR'], df_subset_interp['HRD'], df_subset_interp['CNC'], df_subset_interp['ZDEN'], df_subset_interp['PE_est'], df_subset_interp['DTC_spliced'], df_subset_interp['DTS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"fe-dts\"></a> 5c. Feature Engineering - DTS Approximation\n",
    "\n",
    "back to [Feature Engineering](#fe)\n",
    "\n",
    "back to [top](#top)\n",
    "\n",
    "Finally, there is a missing section of DTS that needs to be approximated.\n",
    "\n",
    "1. For self-consistency with DTC estimation, first build a [Linear Regression](#lr-dts) for DTS on to CNC.\n",
    "1. <a name=\"empirical-dts\"></a>Empirical Models\n",
    "    * [Castagna *et al.* (1993)](#castagna-1993): \n",
    "    ** Water Saturated Sands: $V_{s} (km/s) = 0.804V_{p} - 0.856$\n",
    "    ** Mudrock line: $V_{s} (km/s) = 0.862V_{p} - 1.172$\n",
    "    * [Brocher (2005)](#brocher-2005): $V_{s} (km/s) = 0.7858 - 1.2344V_{p} + 0.7949V_{p}^2 - 0.1238V_{p}^3 + 0.0064V_{p}^4$\n",
    "    * Castanga *et al.* (1985): \n",
    "    ** Shaley sands (Frio formation): $V_{s} (km/s) = 3.89 - 7.07{phi} - 2.04C$, (km/sec) where ${phi}$ is porosity & $C$ is clay fraction\n",
    "    ** Mudrock: $V_{s} (km/s) = 0.862V_{p} - 1.172$\n",
    "    \n",
    "    \n",
    "For more on empirical models for Vp, Vs, & more, see __The Rock Physics Handbook, 2e__ by Mavko *et al.* (2009).  It is always a handy reference in times of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced'])/(1e6/df_subset_interp['DTS']), 'k')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('Vp/Vs ratio')\n",
    "ax.set_title('Vp/Vs ratio', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vp/Vs ratio is anomalously high in the \"shallow\" section until about sample number 10,000 at which it averages about 1.7 - 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare CNC & DTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=2, label='DTS')\n",
    "ax1 = ax.twinx()\n",
    "ax1.plot(df_subset_interp['CNC'], 'r', lw=1, label='CNC')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('DTS')\n",
    "ax1.set_ylabel('CNC', fontdict={'color':'r'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a <a name=\"lr-dts\"></a>Linear Regression model to predict DTS from CNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df_subset_interp['CNC']).reshape(-1,1), np.array(df_subset_interp['DTS']).reshape(-1,1), test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR coeff: {}'.format(lr.coef_))\n",
    "print('LR intercept: {}'.format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR training score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('LR test score :{}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTS_cnc = lr.predict(np.array(df_subset_interp['CNC']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(df_subset_interp['DTS'], DTS_cnc, feat_name='DTS', split='Full Log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from CNC: {}'.format(evaluate_rmse(df_subset_interp['DTS'], DTS_cnc[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an underwhelming result.  Whereas CNC was a relatively robust predictorfor DTC, the relationship does not seem to hold as well for DTS.\n",
    "\n",
    "Instead, there are several well-known [empirical models](#empirical-dts) which we can test to see if we can produce a better estimate of Vs for the interpolated section.\n",
    "\n",
    "Let's first test the well-known and widely applied <a name=\"castagna-1993\"></a>Castana *et al.* (1993):\n",
    "\n",
    "* For water saturated sandstones: $V_{s} (km/s) = 0.804V_{p} - 0.856$, and\n",
    "\n",
    "* The famous \"Mudrock\" line: $V_{s} (km/s) = 0.862V_{p} - 1.172$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_c93ss = 1000*(0.804*((1e6/df_subset_interp['DTC_spliced']/3.281)/1000) - 0.856)\n",
    "Vs_c93mr = 1000*(0.862*((1e6/df_subset_interp['DTC_spliced']/3.281)/1000) - 1.172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS')\n",
    "ax.plot(1e6/(3.281*Vs_c93ss), 'r', lw=1, label='Vs Castagna et al. (1993) Sandstone')\n",
    "ax.plot(1e6/(3.281*Vs_c93mr), 'b', lw=1, label='Vs Castagna et al. (1993) Mudrock')\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('Vs (m/s)')\n",
    "ax.set_title('Comparison of DTS_raw & Castagna et al. (1993)')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Castagna et al. 1993 sandstone: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93ss))))\n",
    "print('RMSE predicting DTS from Castagna et al. 1993 mudrock: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93mr))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On the whole, this is not a drastically different from the linear regression of DTS onto CNC, but there are several reasons why.  The lithology is not simply all wet sandstone or all wet mudrock.  Using the wet sandstone line performs slightly worse than the CNC-model while the mudrock line performs slighty better.\n",
    "\n",
    "* To improve the model fit, we would need to know about the lithology.  \n",
    "\n",
    "* First, let's take a look at the Vp/Vs ratio using these two estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot( df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced'])/(1e6/df_subset_interp['DTS']), 'k', lw=3, label='Vp/Vs raw')\n",
    "ax.plot(df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced']/3.281)/Vs_c93ss, 'r', lw=1, label='Vp/Vs C93SS')\n",
    "ax.plot(df_subset_interp['DTC_spliced'].index, (1e6/df_subset_interp['DTC_spliced']/3.281)/Vs_c93mr, 'b', lw=1, label='Vp/Vs C93MR')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('Vp/Vs ratio')\n",
    "ax.set_title('Vp/Vs ratio', fontdict={'fontsize':20, 'fontweight':'bold'})\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Castagna *et al.* (1993) also has relations for limestones.  As this is the Volve field in the Norwegian North Sea, that is entirely a possibility:\n",
    "\n",
    "$V_{s} (km/s) = -0.055V_{p}^2 + 1.017V_{p} - 1.031$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_c93ls = 1000*(-0.055*np.power(1e6/df_subset_interp['DTC_spliced']/3.281/1000, 2) + 1.017*(1e6/df_subset_interp['DTC_spliced']/3.281/1000) - 1.031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS raw')\n",
    "ax.plot(1e6/(3.281*Vs_c93ss), 'r', lw=1, label='DTS C93 Sandstone')\n",
    "ax.plot(1e6/(3.281*Vs_c93mr), 'b', lw=1, label='DTS C93 Mudrock')\n",
    "ax.plot(1e6/(3.218*Vs_c93ls), 'g', lw=1, label='DTS C93 Limestone')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTS')\n",
    "ax.set_title('Comparison of DTS with Castagna et al. (1993) Mudrock & Limestone relation')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Castagna et al. 1993 limestone: {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(3.281*Vs_c93ls))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... while the overall RMSE of the limestone trend is about the same as the sandstone, there are some zones along the log that this is clearly a better fit, suggesting a substantial proportion of limestone\n",
    "\n",
    "Now, lets try <a name=\"brocher-2005\"></a>Brocher's (2005) relation for Vs from Vp:\n",
    "\n",
    "$V_{s} (km/s) = 0.7858 - 1.2344V_{p} + 0.7949V_{p}^2 - 0.1238V_{p}^3 + 0.0064V_{p}^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease, let's make a variable for Vp\n",
    "Vp_mps = 1e6/df_subset_interp['DTC_spliced']/3.281\n",
    "Vp_kmps = Vp_mps / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs_brocher = 0.7858 - 1.2344*Vp_kmps + 0.7949*np.power(Vp_kmps, 2) - 0.1238*np.power(Vp_kmps, 3) + 0.0064*np.power(Vp_kmps, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_subset_interp['DTS'], 'k', lw=3, label='DTS raw')\n",
    "ax.plot(1e6/(Vs_brocher*1000*3.281), 'r', lw=1, label='DTS Brocher 2005')\n",
    "ax.plot(1e6/(Vs_c93mr*3.281), '--b', lw=1, label='DTS Castagna et al. (1993) mudrock')\n",
    "ax.set_xlabel('sample no')\n",
    "ax.set_ylabel('DTS')\n",
    "ax.set_title('Comparison of Castagna et al. (1993) Mudrock and Brocher (2005)')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE predicting DTS from Brocher (2005): {}'.format(evaluate_rmse(df_subset_interp['DTS'], 1e6/(Vs_brocher*1000*3.281))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brocher (2005) is almost identical to Castagna *et al.* (1993) Mudrock line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdda",
   "language": "python",
   "name": "pdda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
